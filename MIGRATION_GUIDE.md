# Auto-Agent è¿ç§»æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ä» DocHive é¡¹ç›®ä¸­æ‹†åˆ† auto_agent package æ—¶éœ€è¦è¿ç§»å’Œå®ç°çš„åŠŸèƒ½ã€‚

## ç›®å½•

1. [ç¼ºå¤±åŠŸèƒ½æ¸…å•](#1-ç¼ºå¤±åŠŸèƒ½æ¸…å•)
2. [è¿ç§»ä¼˜å…ˆçº§è§„åˆ’](#2-è¿ç§»ä¼˜å…ˆçº§è§„åˆ’)
3. [Phase 1: LLM å®¢æˆ·ç«¯å®ç°](#3-phase-1-llm-å®¢æˆ·ç«¯å®ç°)
4. [Phase 2: æ‰§è¡Œæµç¨‹å¯è§†åŒ–](#4-phase-2-æ‰§è¡Œæµç¨‹å¯è§†åŒ–)
5. [Phase 3: ä¼šè¯ç®¡ç†ç³»ç»Ÿ](#5-phase-3-ä¼šè¯ç®¡ç†ç³»ç»Ÿ)
6. [Phase 4: Agent ç¼–è¾‘ç³»ç»Ÿ](#6-phase-4-agent-ç¼–è¾‘ç³»ç»Ÿ)
7. [Phase 5: æ„å›¾è·¯ç”±ç³»ç»Ÿ](#7-phase-5-æ„å›¾è·¯ç”±ç³»ç»Ÿ)
8. [Phase 6: é«˜çº§åŠŸèƒ½](#8-phase-6-é«˜çº§åŠŸèƒ½)

---

## 1. ç¼ºå¤±åŠŸèƒ½æ¸…å•

| åŠŸèƒ½æ¨¡å—       | å½“å‰çŠ¶æ€     | åŸå§‹ä½ç½®                                          | ä¼˜å…ˆçº§ |
| -------------- | ------------ | ------------------------------------------------- | ------ |
| LLM å®¢æˆ·ç«¯å®ç° | ä»…æœ‰æŠ½è±¡åŸºç±» | `DocHive/backend/utils/llm_client.py`             | P0     |
| æ‰§è¡Œæµç¨‹å¯è§†åŒ– | ç¼ºå¤±         | `DocHive/backend/core/agents/execution_report.py` | P0     |
| ä¼šè¯ç®¡ç†ç³»ç»Ÿ   | ç¼ºå¤±         | `DocHive/backend/core/conversation_manager.py`    | P1     |
| Agent ç¼–è¾‘ç³»ç»Ÿ | ç¼ºå¤±         | `DocHive/backend/core/agent_editor.py`            | P1     |
| æ„å›¾è·¯ç”±ç³»ç»Ÿ   | ç¼ºå¤±         | `DocHive/backend/core/intent_router.py`           | P2     |
| æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç† | éƒ¨åˆ†å®ç°     | `DocHive/backend/core/context.py`                 | P2     |
| é«˜çº§è®°å¿†åŠŸèƒ½   | åŸºç¡€å®ç°     | `DocHive/backend/core/auto_agent/memory/`         | P3     |

---

## 2. è¿ç§»ä¼˜å…ˆçº§è§„åˆ’

### Phase 1 (Week 1-2): åŸºç¡€è®¾æ–½
- [ ] å®Œæ•´çš„ LLM å®¢æˆ·ç«¯å®ç°
- [ ] æ‰§è¡Œæµç¨‹å¯è§†åŒ–ç³»ç»Ÿ

### Phase 2 (Week 3-4): æ ¸å¿ƒåŠŸèƒ½
- [ ] ä¼šè¯ç®¡ç†ç³»ç»Ÿ
- [ ] Agent ç¼–è¾‘å’Œ Markdown è§£æ

### Phase 3 (Week 5-6): å¢å¼ºåŠŸèƒ½
- [ ] æ„å›¾è·¯ç”±ç³»ç»Ÿ
- [ ] æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†

### Phase 4 (Week 7-8): å®Œå–„åŠŸèƒ½
- [ ] é«˜çº§è®°å¿†åŠŸèƒ½
- [ ] æµ‹è¯•å’Œæ–‡æ¡£å®Œå–„

---

## 3. Phase 1: LLM å®¢æˆ·ç«¯å®ç°

### 3.1 ç›®æ ‡
å®ç°å…·ä½“çš„ LLM æä¾›å•†å®¢æˆ·ç«¯ï¼Œæ”¯æŒ OpenAIã€DeepSeek ç­‰ã€‚

### 3.2 æ–‡ä»¶ç»“æ„

```
auto_agent/llm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ client.py          # æŠ½è±¡åŸºç±» (å·²æœ‰)
â”œâ”€â”€ prompts.py         # æç¤ºè¯æ¨¡æ¿ (å·²æœ‰)
â””â”€â”€ providers/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ openai.py      # OpenAI å®ç° (å¾…å®ç°)
    â”œâ”€â”€ deepseek.py    # DeepSeek å®ç° (å¾…å®ç°)
    â””â”€â”€ anthropic.py   # Anthropic å®ç° (å¾…å®ç°)
```

### 3.3 ä»£ç å®ç°

#### 3.3.1 OpenAI å®¢æˆ·ç«¯ (`auto_agent/llm/providers/openai.py`)

```python
"""
OpenAI LLM å®¢æˆ·ç«¯å®ç°
"""
import json
from typing import Any, AsyncGenerator, Dict, List, Optional

import httpx

from auto_agent.llm.client import LLMClient


class OpenAIClient(LLMClient):
    """OpenAI API å®¢æˆ·ç«¯"""

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4o-mini",
        base_url: str = "https://api.openai.com/v1",
        timeout: float = 60.0,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self._client = httpx.AsyncClient(
            timeout=timeout,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
        )

    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs,
    ) -> str:
        """åŒæ­¥èŠå¤©è¡¥å…¨"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
        }
        if max_tokens:
            payload["max_tokens"] = max_tokens
        payload.update(kwargs)

        response = await self._client.post(
            f"{self.base_url}/chat/completions",
            json=payload,
        )
        response.raise_for_status()
        data = response.json()
        return data["choices"][0]["message"]["content"]

    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©è¡¥å…¨"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "stream": True,
        }
        if max_tokens:
            payload["max_tokens"] = max_tokens
        payload.update(kwargs)

        async with self._client.stream(
            "POST",
            f"{self.base_url}/chat/completions",
            json=payload,
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data)
                        content = chunk["choices"][0]["delta"].get("content", "")
                        if content:
                            yield content
                    except json.JSONDecodeError:
                        continue

    async def function_call(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict[str, Any]],
        tool_choice: str = "auto",
        **kwargs,
    ) -> Dict[str, Any]:
        """Function Calling æ”¯æŒ"""
        payload = {
            "model": self.model,
            "messages": messages,
            "tools": tools,
            "tool_choice": tool_choice,
        }
        payload.update(kwargs)

        response = await self._client.post(
            f"{self.base_url}/chat/completions",
            json=payload,
        )
        response.raise_for_status()
        data = response.json()
        
        message = data["choices"][0]["message"]
        if message.get("tool_calls"):
            return {
                "type": "tool_call",
                "tool_calls": message["tool_calls"],
            }
        return {
            "type": "message",
            "content": message.get("content", ""),
        }

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self._client.aclose()
```

#### 3.3.2 DeepSeek å®¢æˆ·ç«¯ (`auto_agent/llm/providers/deepseek.py`)

```python
"""
DeepSeek LLM å®¢æˆ·ç«¯å®ç° (å…¼å®¹ OpenAI API)
"""
from auto_agent.llm.providers.openai import OpenAIClient


class DeepSeekClient(OpenAIClient):
    """DeepSeek API å®¢æˆ·ç«¯ (å…¼å®¹ OpenAI API)"""

    def __init__(
        self,
        api_key: str,
        model: str = "deepseek-chat",
        base_url: str = "https://api.deepseek.com/v1",
        timeout: float = 60.0,
    ):
        super().__init__(
            api_key=api_key,
            model=model,
            base_url=base_url,
            timeout=timeout,
        )
```

#### 3.3.3 æ›´æ–° LLM åŸºç±» (`auto_agent/llm/client.py`)

```python
"""
LLM å®¢æˆ·ç«¯æŠ½è±¡ - å¢å¼ºç‰ˆ
"""
from abc import ABC, abstractmethod
from typing import Any, AsyncGenerator, Dict, List, Optional


class LLMClient(ABC):
    """LLM å®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs,
    ) -> str:
        """èŠå¤©è¡¥å…¨"""
        pass

    @abstractmethod
    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©è¡¥å…¨"""
        pass

    async def function_call(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict[str, Any]],
        tool_choice: str = "auto",
        **kwargs,
    ) -> Dict[str, Any]:
        """Function Calling (å¯é€‰å®ç°)"""
        raise NotImplementedError("This provider does not support function calling")

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        pass

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
```

---

## 4. Phase 2: æ‰§è¡Œæµç¨‹å¯è§†åŒ–

### 4.1 ç›®æ ‡
å®ç°æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨ï¼Œæ”¯æŒ Mermaid æµç¨‹å›¾ã€Markdown/HTML æŠ¥å‘Šå¯¼å‡ºã€‚

### 4.2 æ–‡ä»¶ç»“æ„

```
auto_agent/core/
â”œâ”€â”€ ...
â””â”€â”€ report/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ generator.py      # æŠ¥å‘Šç”Ÿæˆå™¨
    â”œâ”€â”€ mermaid.py        # Mermaid å›¾ç”Ÿæˆ
    â””â”€â”€ templates/        # æŠ¥å‘Šæ¨¡æ¿
        â”œâ”€â”€ markdown.py
        â””â”€â”€ html.py
```

### 4.3 ä»£ç å®ç°

#### 4.3.1 æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨ (`auto_agent/core/report/generator.py`)

```python
"""
æ™ºèƒ½ä½“æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨
"""
import json
from datetime import datetime
from typing import Any, Dict, List, Optional

from auto_agent.models import ExecutionPlan, SubTaskResult


class ExecutionReportGenerator:
    """æ™ºèƒ½ä½“æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨"""

    @staticmethod
    def generate_report_data(
        agent_name: str,
        query: str,
        plan: ExecutionPlan,
        results: List[SubTaskResult],
        state: Dict[str, Any],
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»“æ„åŒ–çš„æ‰§è¡ŒæŠ¥å‘Šæ•°æ®

        Args:
            agent_name: æ™ºèƒ½ä½“åç§°
            query: ç”¨æˆ·æŸ¥è¯¢
            plan: æ‰§è¡Œè®¡åˆ’
            results: æ‰§è¡Œç»“æœåˆ—è¡¨
            state: æœ€ç»ˆçŠ¶æ€
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            ç»“æ„åŒ–çš„æŠ¥å‘Šæ•°æ®
        """
        # æ„å»ºæ­¥éª¤æ‰§è¡ŒçŠ¶æ€æ˜ å°„
        result_map = {r.step_id: r for r in results}

        # æ„å»ºæ­¥éª¤è¯¦æƒ…
        steps_detail = []
        for step in plan.subtasks:
            result = result_map.get(step.id)
            
            if result is None:
                status = "pending"
            elif result.success:
                status = "success"
            else:
                status = "failed"

            steps_detail.append({
                "step": step.id,
                "name": step.tool,
                "description": step.description,
                "expectations": step.expectations,
                "status": status,
                "output": ExecutionReportGenerator._compress_output(
                    result.output if result else None
                ),
                "error": result.error if result and not result.success else None,
            })

        # ç»Ÿè®¡ä¿¡æ¯
        total_steps = len(plan.subtasks)
        executed_steps = len(results)
        successful_steps = sum(1 for r in results if r.success)
        failed_steps = executed_steps - successful_steps

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        duration = None
        if start_time and end_time:
            duration = (end_time - start_time).total_seconds()

        return {
            "agent_name": agent_name,
            "query": query[:500] + "..." if len(query) > 500 else query,
            "generated_at": datetime.now().isoformat(),
            "start_time": start_time.isoformat() if start_time else None,
            "end_time": end_time.isoformat() if end_time else None,
            "duration_seconds": duration,
            "statistics": {
                "total_steps": total_steps,
                "executed_steps": executed_steps,
                "successful_steps": successful_steps,
                "failed_steps": failed_steps,
                "success_rate": round(
                    successful_steps / executed_steps * 100, 1
                ) if executed_steps > 0 else 0,
            },
            "steps": steps_detail,
            "final_state": ExecutionReportGenerator._compress_state(state),
            "mermaid_diagram": ExecutionReportGenerator.generate_mermaid(
                plan, results
            ),
        }

    @staticmethod
    def generate_mermaid(
        plan: ExecutionPlan,
        results: List[SubTaskResult],
    ) -> str:
        """ç”Ÿæˆ Mermaid æµç¨‹å›¾"""
        result_map = {r.step_id: r for r in results}
        
        lines = ["graph TD"]
        lines.append("    Start([å¼€å§‹]) --> Step1")

        for i, step in enumerate(plan.subtasks):
            step_id = f"Step{step.id}"
            result = result_map.get(step.id)
            
            # ç¡®å®šèŠ‚ç‚¹æ ·å¼
            if result is None:
                style = "pending"
                shape_start, shape_end = "[", "]"
            elif result.success:
                style = "success"
                shape_start, shape_end = "[", "]"
            else:
                style = "failed"
                shape_start, shape_end = "[[", "]]"

            # èŠ‚ç‚¹æ ‡ç­¾
            tool_name = step.tool or "unknown"
            label = f"{tool_name}"
            lines.append(f"    {step_id}{shape_start}{label}{shape_end}")

            # è¿æ¥åˆ°ä¸‹ä¸€æ­¥
            if i < len(plan.subtasks) - 1:
                next_id = f"Step{plan.subtasks[i + 1].id}"
                lines.append(f"    {step_id} --> {next_id}")
            else:
                lines.append(f"    {step_id} --> End([ç»“æŸ])")

        # æ·»åŠ æ ·å¼
        lines.append("")
        for step in plan.subtasks:
            result = result_map.get(step.id)
            step_id = f"Step{step.id}"
            if result is None:
                lines.append(f"    style {step_id} fill:#gray")
            elif result.success:
                lines.append(f"    style {step_id} fill:#90EE90")
            else:
                lines.append(f"    style {step_id} fill:#FFB6C1")

        return "\n".join(lines)

    @staticmethod
    def generate_markdown_report(report_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        lines = [
            f"# æ™ºèƒ½ä½“æ‰§è¡ŒæŠ¥å‘Š",
            f"",
            f"**Agent**: {report_data['agent_name']}",
            f"**æ‰§è¡Œæ—¶é—´**: {report_data['generated_at']}",
            f"**ç”¨æˆ·è¾“å…¥**: {report_data['query']}",
            f"",
            f"---",
            f"",
            f"## æ‰§è¡Œç»Ÿè®¡",
            f"",
            f"| æŒ‡æ ‡ | å€¼ |",
            f"|------|-----|",
            f"| æ€»æ­¥éª¤æ•° | {report_data['statistics']['total_steps']} |",
            f"| å·²æ‰§è¡Œ | {report_data['statistics']['executed_steps']} |",
            f"| æˆåŠŸ | {report_data['statistics']['successful_steps']} |",
            f"| å¤±è´¥ | {report_data['statistics']['failed_steps']} |",
            f"| æˆåŠŸç‡ | {report_data['statistics']['success_rate']}% |",
            f"",
            f"## æ‰§è¡Œæµç¨‹",
            f"",
            f"```mermaid",
            report_data['mermaid_diagram'],
            f"```",
            f"",
            f"## æ­¥éª¤è¯¦æƒ…",
            f"",
        ]

        for step in report_data['steps']:
            status_icon = {
                "success": "âœ…",
                "failed": "âŒ",
                "pending": "â³",
            }.get(step['status'], "â“")
            
            lines.append(f"### {status_icon} æ­¥éª¤ {step['step']}: {step['name']}")
            lines.append(f"")
            lines.append(f"- **æè¿°**: {step['description']}")
            if step['expectations']:
                lines.append(f"- **æœŸæœ›**: {step['expectations']}")
            lines.append(f"- **çŠ¶æ€**: {step['status']}")
            if step['error']:
                lines.append(f"- **é”™è¯¯**: {step['error']}")
            lines.append(f"")

        return "\n".join(lines)

    @staticmethod
    def _compress_output(output: Any) -> Any:
        """å‹ç¼©è¾“å‡ºæ•°æ®"""
        if output is None:
            return None
        if isinstance(output, dict):
            compressed = {}
            for k, v in output.items():
                if k == "documents" and isinstance(v, list):
                    compressed[k] = f"[{len(v)} documents]"
                elif isinstance(v, str) and len(v) > 200:
                    compressed[k] = v[:200] + "..."
                else:
                    compressed[k] = v
            return compressed
        return output

    @staticmethod
    def _compress_state(state: Dict[str, Any]) -> Dict[str, Any]:
        """å‹ç¼©çŠ¶æ€æ•°æ®"""
        compressed = {}
        for k, v in state.items():
            if k in ["inputs", "control"]:
                compressed[k] = v
            elif isinstance(v, list) and len(v) > 5:
                compressed[k] = f"[{len(v)} items]"
            elif isinstance(v, dict) and len(str(v)) > 500:
                compressed[k] = f"{{...{len(v)} keys}}"
            else:
                compressed[k] = v
        return compressed
```


---

## 5. Phase 3: ä¼šè¯ç®¡ç†ç³»ç»Ÿ

### 5.1 ç›®æ ‡
å®ç°å¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒä¼šè¯æŒä¹…åŒ–å’Œç”¨æˆ·å¹²é¢„ã€‚

### 5.2 æ–‡ä»¶ç»“æ„

```
auto_agent/
â”œâ”€â”€ session/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py        # ä¼šè¯ç®¡ç†å™¨
â”‚   â”œâ”€â”€ models.py         # ä¼šè¯æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ memory.py     # å†…å­˜å­˜å‚¨
â”‚       â”œâ”€â”€ sqlite.py     # SQLite å­˜å‚¨
â”‚       â””â”€â”€ redis.py      # Redis å­˜å‚¨
```

### 5.3 ä»£ç å®ç°

```python
"""
ä¼šè¯ç®¡ç†å™¨ (auto_agent/session/manager.py)
"""
import asyncio
import time
import uuid
from typing import Any, Dict, List, Optional

from auto_agent.models import Message


class SessionManager:
    """
    ä¼šè¯ç®¡ç†å™¨
    
    ç‰¹æ€§:
    1. åŸºäº session_id ç®¡ç†ä¼šè¯çŠ¶æ€
    2. æ”¯æŒå¤šè½®å¯¹è¯
    3. æ”¯æŒç”¨æˆ·å¹²é¢„ï¼ˆç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼‰
    4. è‡ªåŠ¨è¿‡æœŸæ¸…ç†
    """

    def __init__(self, default_ttl: int = 1800):
        self._sessions: Dict[str, Dict[str, Any]] = {}
        self._default_ttl = default_ttl
        self._cleanup_task = None

    def create_session(
        self,
        user_id: str,
        initial_query: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        session_id = str(uuid.uuid4())
        current_time = int(time.time())

        self._sessions[session_id] = {
            "session_id": session_id,
            "user_id": user_id,
            "created_at": current_time,
            "updated_at": current_time,
            "expires_at": current_time + self._default_ttl,
            "status": "active",  # active / waiting_input / completed / error
            "messages": [Message(role="user", content=initial_query, timestamp=current_time)],
            "state": {},
            "metadata": metadata or {},
        }
        return session_id

    def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä¼šè¯"""
        session = self._sessions.get(session_id)
        if session and int(time.time()) < session["expires_at"]:
            return session
        return None

    def update_session(
        self,
        session_id: str,
        status: Optional[str] = None,
        state: Optional[Dict[str, Any]] = None,
        message: Optional[Message] = None,
    ) -> bool:
        """æ›´æ–°ä¼šè¯"""
        session = self._sessions.get(session_id)
        if not session:
            return False

        current_time = int(time.time())
        session["updated_at"] = current_time
        session["expires_at"] = current_time + self._default_ttl

        if status:
            session["status"] = status
        if state:
            session["state"].update(state)
        if message:
            session["messages"].append(message)

        return True

    def wait_for_input(self, session_id: str, prompt: str) -> bool:
        """è®¾ç½®ä¼šè¯ä¸ºç­‰å¾…ç”¨æˆ·è¾“å…¥çŠ¶æ€"""
        return self.update_session(
            session_id,
            status="waiting_input",
            state={"waiting_prompt": prompt},
        )

    def resume_session(self, session_id: str, user_input: str) -> bool:
        """æ¢å¤ä¼šè¯ï¼ˆç”¨æˆ·æä¾›è¾“å…¥åï¼‰"""
        session = self.get_session(session_id)
        if not session or session["status"] != "waiting_input":
            return False

        return self.update_session(
            session_id,
            status="active",
            message=Message(role="user", content=user_input, timestamp=int(time.time())),
        )

    def close_session(self, session_id: str, status: str = "completed") -> bool:
        """å…³é—­ä¼šè¯"""
        return self.update_session(session_id, status=status)

    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        current_time = int(time.time())
        expired = [
            sid for sid, s in self._sessions.items()
            if current_time > s["expires_at"]
        ]
        for sid in expired:
            del self._sessions[sid]
        return len(expired)
```

---

## 6. Phase 4: Agent ç¼–è¾‘ç³»ç»Ÿ

### 6.1 ç›®æ ‡
æ”¯æŒä½¿ç”¨ Markdown å®šä¹‰ Agentï¼Œè‡ªåŠ¨è§£æä¸ºç»“æ„åŒ–é…ç½®ã€‚

### 6.2 å·²å®ç°åŠŸèƒ½

- âœ… `AgentMarkdownParser`: Markdown è§£æå™¨
- âœ… `AgentDefinition`: Agent å®šä¹‰æ•°æ®ç»“æ„
- âœ… è§„åˆ™è§£æ (æ—  LLM é™çº§æ–¹æ¡ˆ)
- âœ… LLM è§£æ (æ™ºèƒ½ç†è§£è‡ªç„¶è¯­è¨€)

### 6.3 ä½¿ç”¨ç¤ºä¾‹

```python
from auto_agent import AgentMarkdownParser, OpenAIClient

# å®šä¹‰ Agent (Markdown æ ¼å¼)
agent_md = """
## æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“

ä½ éœ€è¦æŒ‰ä»¥ä¸‹æ­¥éª¤å®Œæˆç”¨æˆ·çš„éœ€æ±‚ï¼š

1. è°ƒç”¨ [analyze_input] å·¥å…·ï¼Œåˆ†æç”¨æˆ·æ„å›¾
2. è°ƒç”¨ [es_fulltext_search] å·¥å…·ï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£
3. è°ƒç”¨ [generate_outline] å·¥å…·ï¼Œç”Ÿæˆå¤§çº²
4. è°ƒç”¨ [document_compose] å·¥å…·ï¼Œæ’°å†™æ–‡æ¡£

### ç›®æ ‡
- ç†è§£ç”¨æˆ·çš„å†™ä½œéœ€æ±‚
- ç”Ÿæˆç»“æ„æ¸…æ™°çš„æ–‡æ¡£

### çº¦æŸ
- æ–‡æ¡£é•¿åº¦ä¸è¶…è¿‡5000å­—
"""

# è§£æ
llm = OpenAIClient(api_key="sk-xxx")
parser = AgentMarkdownParser(llm_client=llm)
result = await parser.parse(agent_md)

if result["success"]:
    agent_def = result["agent"]
    print(f"Agent: {agent_def.name}")
    print(f"Goals: {agent_def.goals}")
    print(f"Steps: {[s.tool for s in agent_def.initial_plan]}")
```

---

## 7. Phase 5: æ„å›¾è·¯ç”±ç³»ç»Ÿ

### 7.1 ç›®æ ‡
å®ç°æ™ºèƒ½æ„å›¾è¯†åˆ«ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªåŠ¨é€‰æ‹©æ‰§è¡Œè·¯å¾„ã€‚

### 7.2 æ–‡ä»¶ç»“æ„

```
auto_agent/core/
â”œâ”€â”€ router/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ intent.py         # æ„å›¾è¯†åˆ«å™¨
â”‚   â””â”€â”€ dispatcher.py     # è·¯ç”±åˆ†å‘å™¨
```

### 7.3 ä»£ç å®ç° (å¾…å®ç°)

```python
"""
æ„å›¾è·¯ç”±å™¨ (auto_agent/core/router/intent.py)
"""
from typing import Any, Dict, List, Optional

from auto_agent.llm.client import LLMClient


class IntentRouter:
    """
    æ„å›¾è·¯ç”±å™¨
    
    åŠŸèƒ½:
    1. è¯†åˆ«ç”¨æˆ·æ„å›¾
    2. é€‰æ‹©åˆé€‚çš„ Agent æˆ–å·¥å…·
    3. æ”¯æŒ Function Calling æ¨¡å¼
    """

    INTENT_PROMPT = '''åˆ†æç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«æ„å›¾å¹¶é€‰æ‹©åˆé€‚çš„å¤„ç†æ–¹å¼ã€‚

ç”¨æˆ·è¾“å…¥: {query}

å¯ç”¨çš„å¤„ç†æ–¹å¼:
{handlers}

è¿”å› JSON:
{{
    "intent": "æ„å›¾æè¿°",
    "handler": "å¤„ç†æ–¹å¼åç§°",
    "confidence": 0.95,
    "parameters": {{}}
}}
'''

    def __init__(
        self,
        llm_client: LLMClient,
        handlers: Dict[str, Dict[str, Any]],
    ):
        self.llm_client = llm_client
        self.handlers = handlers

    async def route(self, query: str) -> Dict[str, Any]:
        """è·¯ç”±ç”¨æˆ·è¯·æ±‚"""
        # æ„å»ºå¤„ç†æ–¹å¼æè¿°
        handlers_desc = "\n".join([
            f"- {name}: {h.get('description', '')}"
            for name, h in self.handlers.items()
        ])

        prompt = self.INTENT_PROMPT.format(
            query=query,
            handlers=handlers_desc,
        )

        response = await self.llm_client.chat([
            {"role": "user", "content": prompt}
        ])

        # è§£æå“åº”
        import json
        import re
        
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        
        return {
            "intent": "unknown",
            "handler": "default",
            "confidence": 0.5,
        }
```

---

## 8. Phase 6: é«˜çº§åŠŸèƒ½

### 8.1 å¾…å®ç°åŠŸèƒ½æ¸…å•

| åŠŸèƒ½         | æè¿°                        | ä¼˜å…ˆçº§ |
| ------------ | --------------------------- | ------ |
| å‘é‡è®°å¿†æ£€ç´¢ | åŸºäº embedding çš„ç›¸ä¼¼åº¦æœç´¢ | P2     |
| æ‰§è¡Œå›æ”¾     | é‡æ”¾å†å²æ‰§è¡Œè¿‡ç¨‹            | P3     |
| å¹¶è¡Œæ‰§è¡Œ     | æ”¯æŒå¹¶è¡Œæ‰§è¡Œæ— ä¾èµ–çš„æ­¥éª¤    | P2     |
| æ‰§è¡Œç›‘æ§     | å®æ—¶ç›‘æ§æ‰§è¡ŒçŠ¶æ€            | P3     |
| æ’ä»¶ç³»ç»Ÿ     | æ”¯æŒç¬¬ä¸‰æ–¹æ’ä»¶æ‰©å±•          | P3     |

### 8.2 å‘é‡è®°å¿†æ£€ç´¢ (ç¤ºä¾‹)

```python
"""
å‘é‡è®°å¿† (auto_agent/memory/vector.py)
"""
from typing import Any, Dict, List, Optional


class VectorMemory:
    """
    åŸºäºå‘é‡çš„è®°å¿†ç³»ç»Ÿ
    
    æ”¯æŒ:
    - æ–‡æœ¬ embedding
    - ç›¸ä¼¼åº¦æœç´¢
    - è®°å¿†èšç±»
    """

    def __init__(
        self,
        embedding_model: str = "text-embedding-3-small",
        dimension: int = 1536,
    ):
        self.embedding_model = embedding_model
        self.dimension = dimension
        self._vectors: List[Dict[str, Any]] = []

    async def add(
        self,
        content: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """æ·»åŠ è®°å¿†"""
        # TODO: è°ƒç”¨ embedding API
        vector_id = f"vec_{len(self._vectors)}"
        self._vectors.append({
            "id": vector_id,
            "content": content,
            "metadata": metadata or {},
            "embedding": [],  # å®é™…åº”è¯¥æ˜¯ embedding å‘é‡
        })
        return vector_id

    async def search(
        self,
        query: str,
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼è®°å¿†"""
        # TODO: å®ç°å‘é‡ç›¸ä¼¼åº¦æœç´¢
        return self._vectors[:top_k]
```

---

## 9. æµ‹è¯•ç”¨ä¾‹

### 9.1 å®Œæ•´ç¤ºä¾‹: æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“

å‚è§ `examples/writer_agent_demo.py`

è¿è¡Œæ–¹å¼:

```bash
# å®Œæ•´æ¨¡å¼ (éœ€è¦ API Key)
export OPENAI_API_KEY=sk-xxx
python examples/writer_agent_demo.py

# ç®€åŒ–æ¨¡å¼ (æ— éœ€ API Key)
python examples/writer_agent_demo.py --simple
```

### 9.2 é¢„æœŸè¾“å‡º

```
============================================================
ğŸ“ æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“ç¤ºä¾‹
============================================================
âœ… ä½¿ç”¨ LLM: gpt-4o-mini
âœ… å·²æ³¨å†Œ 4 ä¸ªå·¥å…·
âœ… Agent è§£ææˆåŠŸ: æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“
   ç›®æ ‡: ['ç†è§£ç”¨æˆ·çš„å†™ä½œéœ€æ±‚', 'æ£€ç´¢ç›¸å…³å‚è€ƒèµ„æ–™', 'ç”Ÿæˆç»“æ„æ¸…æ™°çš„æ–‡æ¡£']
   çº¦æŸ: ['æ–‡æ¡£é•¿åº¦é€‚ä¸­ï¼Œä¸è¶…è¿‡5000å­—', 'å¼•ç”¨çš„å‚è€ƒèµ„æ–™ä¸è¶…è¿‡10ç¯‡']
   æ­¥éª¤æ•°: 4
âœ… Agent åˆå§‹åŒ–å®Œæˆ

ğŸ“‹ ç”¨æˆ·æŸ¥è¯¢: å¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„è°ƒç ”æŠ¥å‘Š
------------------------------------------------------------

âœ… æ‰§è¡Œå®Œæˆ!
   ä¼šè¯ID: xxx-xxx-xxx
   è€—æ—¶: 2.35 ç§’

============================================================
ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š
============================================================
# æ™ºèƒ½ä½“æ‰§è¡ŒæŠ¥å‘Š

**Agent**: æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“
...
```

---

## 10. è¿ç§»æ£€æŸ¥æ¸…å•

### Phase 1 âœ… å®Œæˆ
- [x] OpenAI å®¢æˆ·ç«¯å®ç° (`auto_agent/llm/providers/openai.py`)
- [x] æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨ (`auto_agent/core/report/generator.py`)
- [x] Mermaid æµç¨‹å›¾ç”Ÿæˆ

### Phase 2 âœ… å®Œæˆ
- [x] Agent Markdown è§£æå™¨ (`auto_agent/core/editor/parser.py`)
- [x] AgentDefinition æ•°æ®ç»“æ„
- [x] ä¼šè¯ç®¡ç†å™¨ (`auto_agent/session/manager.py`)

### Phase 3 âœ… å®Œæˆ
- [x] æ„å›¾è·¯ç”±ç³»ç»Ÿ (`auto_agent/core/router/intent.py`)
- [x] åˆ†ç±»è®°å¿†ç³»ç»Ÿ (`auto_agent/memory/categorized.py`)
- [x] KV é”®å€¼å¯¹å­˜å‚¨
- [x] å…¨æ–‡æ£€ç´¢æ”¯æŒ

### Phase 4 âœ… å®Œæˆ
- [x] å®Œæ•´æµ‹è¯•è¦†ç›– (49 ä¸ªæµ‹è¯•ç”¨ä¾‹)
- [x] é›†æˆæµ‹è¯• (`tests/test_integration.py`)

---

## 11. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¯ç”¨**: è¿è¡Œ `examples/writer_agent_demo.py --simple` æµ‹è¯•åŸºç¡€åŠŸèƒ½
2. **é…ç½® API**: è®¾ç½® `OPENAI_API_KEY` ç¯å¢ƒå˜é‡å¯ç”¨å®Œæ•´åŠŸèƒ½
3. **è‡ªå®šä¹‰å·¥å…·**: å‚è€ƒç¤ºä¾‹åˆ›å»ºè‡ªå·±çš„å·¥å…·
4. **æ‰©å±•åŠŸèƒ½**: æŒ‰éœ€å®ç° Phase 3-4 çš„åŠŸèƒ½
