"""
OpenAI åŸç”Ÿå®¢æˆ·ç«¯ç‰ˆæœ¬ Adaptive Memo Demoï¼ˆtools-callï¼‰

ä¸ examples/adaptive_memo_demo.py å¯¹æ¯”ï¼šä½¿ç”¨ OpenAI åŸç”Ÿ function calling å®ç°ï¼ˆgenerate / verify / reflect ä¸‰å·¥å…·é—­ç¯ï¼‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    cd auto_agent
    python examples/langchain_compare/adaptive_memo_langchain.py
"""

import asyncio
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° pathï¼ˆä»¿ç…§ examples/langchain_compare/main.pyï¼‰
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root))

from openai import AsyncOpenAI


def _json_extract(text: str) -> Optional[Dict[str, Any]]:
    """ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON objectï¼ˆå®½æ¾è§£æï¼‰"""
    try:
        m = re.search(r"\{[\s\S]*\}", text or "")
        if not m:
            return None
        return json.loads(m.group(0))
    except Exception:
        return None


# ==================== Token è¿½è¸ªï¼ˆä»¿ç…§ main.pyï¼‰ ====================


class TokenTracker:
    """Token è¿½è¸ªå™¨"""

    def __init__(self):
        self.steps: List[Dict[str, Any]] = []
        self.cumulative_tokens = 0
        self.llm_call_count = 0

    def add(self, tokens: int, step_name: str):
        self.llm_call_count += 1
        self.cumulative_tokens += tokens
        self.steps.append(
            {
                "step": step_name,
                "tokens": tokens,
                "cumulative": self.cumulative_tokens,
            }
        )
        print(f"   ğŸ“Š Token: +{tokens:,} | ç´¯è®¡: {self.cumulative_tokens:,}")


# ==================== å·¥å…·å®šä¹‰ï¼ˆ3 toolsï¼‰ ====================


TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "generate_memo_tool",
            "description": "æ ¹æ® requirements + rubric (+feedback/+previous_memo) ç”Ÿæˆå¤‡å¿˜å½•æ­£æ–‡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "requirements": {"type": "string", "description": "ä»»åŠ¡èƒŒæ™¯ä¸çº¦æŸ"},
                    "rubric": {
                        "type": "object",
                        "description": "è¯„ä¼°ä¸å†™ä½œè§„åˆ™(JSON)ã€‚è‹¥ç¼ºå¤±ï¼Œå·¥å…·å°†è‡ªè¡Œæ¨æ–­å¹¶è¡¥å…¨ã€‚",
                    },
                    "feedback": {
                        "type": "string",
                        "description": "ä¸Šä¸€è½®åæ€å¾—åˆ°çš„å¯æ‰§è¡Œæ”¹è¿›æŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰",
                        "default": "",
                    },
                    "previous_memo": {
                        "type": "string",
                        "description": "ä¸Šä¸€ç‰ˆå¤‡å¿˜å½•ï¼ˆå¯é€‰ï¼Œç”¨äºå¢é‡æ”¹å†™ï¼‰",
                        "default": "",
                    },
                },
                "required": ["requirements"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "verify_memo_tool",
            "description": "LLM judgeï¼šéªŒè¯ memo æ˜¯å¦æ»¡è¶³ rubricï¼Œè¿”å› passed/issues/score ç­‰ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "memo": {"type": "string", "description": "å¾…éªŒè¯çš„ memoï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.memoï¼‰"},
                    "rubric": {"type": "object", "description": "rubricï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.rubricï¼‰"},
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "reflect_feedback_tool",
            "description": "åŸºäº verification æŠ¥å‘Šç”Ÿæˆä¸‹ä¸€è½®å¯æ‰§è¡Œæ”¹è¿›æŒ‡ä»¤ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "requirements": {
                        "type": "string",
                        "description": "åŸå§‹ requirementsï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.requirementsï¼‰",
                    },
                    "memo": {"type": "string", "description": "ä¸Šä¸€ç‰ˆ memoï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.memoï¼‰"},
                    "verification": {
                        "type": "object",
                        "description": "verify_memo_tool è¾“å‡ºï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.verificationï¼‰",
                    },
                    "rubric": {"type": "object", "description": "rubricï¼ˆç¼ºå¤±åˆ™é»˜è®¤ç”¨å½“å‰ state.rubricï¼‰"},
                },
                "required": [],
            },
        },
    },
]


# ==================== å·¥å…·å®ç°ï¼ˆä»¿ç…§ main.py çš„ ToolExecutorï¼‰ ====================


class ToolExecutor:
    """å·¥å…·æ‰§è¡Œå™¨ï¼ˆç»´æŠ¤ stateï¼Œå¹¶åœ¨å·¥å…·å†…éƒ¨è°ƒç”¨ LLMï¼‰"""

    def __init__(self, client: AsyncOpenAI, model: str):
        self.client = client
        self.model = model
        self.state: Dict[str, Any] = {
            "requirements": "",
            "rubric": None,
            "memo": "",
            "verification": None,
            "feedback": "",
        }
        self._fail_once = True  # æ¨¡æ‹Ÿä¸€æ¬¡ç¬æ€å¤±è´¥ï¼Œä¾¿äºå¯¹æ¯” auto_agent çš„ retry

    async def execute(self, tool_name: str, args: Dict[str, Any]) -> str:
        """æ‰§è¡Œå·¥å…·ï¼ˆè¿”å›å­—ç¬¦ä¸² contentï¼Œå†™å…¥ messages['tool']ï¼‰"""
        print(f"   â–¶ï¸ æ‰§è¡Œå·¥å…·: {tool_name}")

        if tool_name == "generate_memo_tool":
            result = await self._generate_memo_tool(args)
        elif tool_name == "verify_memo_tool":
            result = await self._verify_memo_tool(args)
        elif tool_name == "reflect_feedback_tool":
            result = await self._reflect_feedback_tool(args)
        else:
            result = {"success": False, "error": f"æœªçŸ¥å·¥å…·: {tool_name}"}

        return json.dumps(result, ensure_ascii=False)

    async def _infer_rubric(self, requirements: str) -> Dict[str, Any]:
        """rubric çš„ llm_inferï¼ˆåœ¨ tools-call ç‰ˆæœ¬é‡Œæ”¾åˆ°å·¥å…·å†…éƒ¨åšï¼‰"""
        fallback = {
            "required_sections": ["èƒŒæ™¯", "å†³ç­–", "æ–¹æ¡ˆå¯¹æ¯”", "é£é™©æ¸…å•", "ä¸‹ä¸€æ­¥"],
            "min_risks": 5,
            "max_words": 650,
            "tone": "ä¸“ä¸š",
        }

        prompt = f"""è¯·ä¸ºä¸‹é¢çš„å†™ä½œä»»åŠ¡ç”Ÿæˆä¸€ä¸ª rubric(JSON)ï¼Œç”¨äºçº¦æŸâ€œå†³ç­–å¤‡å¿˜å½•â€è¾“å‡ºã€‚

ä½ å¿…é¡»è¾“å‡º JSONï¼ˆä¸è¦è¾“å‡ºå…¶å®ƒæ–‡å­—ï¼‰ï¼Œè‡³å°‘åŒ…å«ï¼š
- required_sections: string[]
- min_risks: int
- max_words: int
- tone: string

ã€requirementsã€‘
{requirements}
"""
        resp = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        txt = resp.choices[0].message.content or ""
        obj = _json_extract(txt) or {}
        if not isinstance(obj, dict):
            return fallback

        required_sections = obj.get("required_sections") or fallback["required_sections"]
        if not isinstance(required_sections, list) or not required_sections:
            required_sections = fallback["required_sections"]

        try:
            min_risks = int(obj.get("min_risks") or fallback["min_risks"])
        except Exception:
            min_risks = fallback["min_risks"]
        try:
            max_words = int(obj.get("max_words") or fallback["max_words"])
        except Exception:
            max_words = fallback["max_words"]
        tone = str(obj.get("tone") or fallback["tone"])

        return {
            "required_sections": [str(x) for x in required_sections if str(x).strip()],
            "min_risks": max(0, min_risks),
            "max_words": max(0, max_words),
            "tone": tone,
        }

    async def _generate_memo_tool(self, args: Dict[str, Any]) -> Dict[str, Any]:
        requirements = (args.get("requirements") or self.state.get("requirements") or "").strip()
        if not requirements:
            return {"success": False, "error": "missing_requirements"}
        self.state["requirements"] = requirements

        rubric = args.get("rubric") or self.state.get("rubric")
        if not isinstance(rubric, dict) or not rubric:
            rubric = await self._infer_rubric(requirements)

        feedback = (args.get("feedback") or self.state.get("feedback") or "").strip()
        previous_memo = (args.get("previous_memo") or self.state.get("memo") or "").strip()

        # æ¨¡æ‹Ÿä¸€æ¬¡ç¬æ€å¤±è´¥ï¼šè®© demo èƒ½çœ‹åˆ°â€œå·¥å…·å¤±è´¥ -> ä¸‹ä¸€è½®é‡è¯•/è°ƒæ•´â€
        if self._fail_once:
            self._fail_once = False
            print("   âš ï¸ æ¨¡æ‹Ÿç¬æ€å¤±è´¥ä¸€æ¬¡ï¼ˆç”¨äºå±•ç¤º retry/replanï¼‰")
            return {"success": False, "error": "transient_failure: simulate retry once"}

        # æŠŠ rubric çš„å…·ä½“æ•°å€¼ç›´æ¥å†™è¿› promptï¼Œè®©æ¨¡å‹æ›´å®¹æ˜“éµå®ˆ
        max_words = int(rubric.get("max_words") or 650)
        min_risks = int(rubric.get("min_risks") or 5)
        required_sections = rubric.get("required_sections") or ["èƒŒæ™¯", "å†³ç­–", "æ–¹æ¡ˆå¯¹æ¯”", "é£é™©æ¸…å•", "ä¸‹ä¸€æ­¥"]
        sections_str = "ã€".join(required_sections)
        tone = rubric.get("tone") or "ä¸“ä¸š"
        _ = rubric  # keep rubric reference

        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±æŠ€æœ¯è´Ÿè´£äººï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚æ’°å†™ä¸€ä»½â€œå†³ç­–å¤‡å¿˜å½•â€(ä¸­æ–‡)ã€‚

ã€requirementsã€‘
{requirements}

ã€ä¸Šä¸€æ¬¡åé¦ˆï¼ˆå¦‚æœæœ‰åˆ™å¿…é¡»é€æ¡è½å®ï¼‰ã€‘
{feedback if feedback else "æ— "}

ã€ä¸Šä¸€ç‰ˆå¤‡å¿˜å½•ï¼ˆå¯é€‰å‚è€ƒï¼Œé¿å…é‡å¤ç©ºè¯ï¼‰ã€‘
{previous_memo[:1500] if previous_memo else "æ— "}

ã€ç¡¬æ€§çº¦æŸï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ã€‘
1) ç« èŠ‚æ ‡é¢˜ï¼šå¿…é¡»åŒ…å« {sections_str}ï¼ˆæŒ‰æ­¤é¡ºåºä½œä¸ºä¸€çº§æ ‡é¢˜ï¼‰
2) é£é™©æ¸…å•ï¼šå¿…é¡»åˆ—å‡º >= {min_risks} æ¡é£é™©ï¼Œæ¯æ¡åŒ…å«: é£é™©/ä¸¥é‡åº¦/ç¼“è§£æªæ–½
3) å­—æ•°é™åˆ¶ï¼šå…¨æ–‡ä¸è¶…è¿‡ {max_words} ä¸ªä¸­æ–‡å­—ç¬¦ï¼ˆå½“å‰é™åˆ¶ {max_words} å­—ï¼Œè¯·ç²¾ç‚¼è¡¨è¾¾ï¼‰
4) è¯­æ°”é£æ ¼ï¼š{tone}
5) ä¸è¦è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºè§£é‡Šï¼Œç›´æ¥è¾“å‡ºå¤‡å¿˜å½•æ­£æ–‡
"""
        resp = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
        )
        memo = (resp.choices[0].message.content or "").strip()

        self.state["rubric"] = rubric
        self.state["memo"] = memo
        if feedback:
            self.state["feedback"] = feedback

        print(f"   âœ… ç”Ÿæˆå®Œæˆ: memo_len={len(memo)}")
        return {
            "success": True,
            "memo": memo,
            "rubric": rubric,
            "used_feedback": feedback or "",
        }

    async def _verify_memo_tool(self, args: Dict[str, Any]) -> Dict[str, Any]:
        memo = (args.get("memo") or self.state.get("memo") or "").strip()
        rubric = args.get("rubric") or self.state.get("rubric") or {}
        if not memo:
            return {"success": True, "passed": False, "issues": ["missing_memo"], "score": 0.0, "verification": {}}
        if not isinstance(rubric, dict) or not rubric:
            rubric = await self._infer_rubric(self.state.get("requirements") or "")

        min_risks = int(rubric.get("min_risks") or 0)
        max_words = int(rubric.get("max_words") or 0)

        rubric_json = json.dumps(rubric, ensure_ascii=False, indent=2)
        # å¯¹é½ examples/adaptive_memo_demo.py çš„ LLM judge prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„â€œå¤‡å¿˜å½•è¯„å®¡å®˜â€ã€‚è¯·ä»…åŸºäº rubric åˆ¤æ–­ memo æ˜¯å¦é€šè¿‡ï¼Œå¹¶ç»™å‡ºå¯æ‰§è¡Œ issuesã€‚

è¦æ±‚ï¼š
1) ä½ å¿…é¡»è¾“å‡º JSONï¼ˆä¸è¦è¾“å‡ºå…¶å®ƒæ–‡å­—ï¼‰ï¼Œç»“æ„å¦‚ä¸‹ï¼š
{{
  "passed": true/false,
  "score": 0-100,
  "issues": ["..."],
  "risk_items": ["..."],  // ä½ è®¤ä¸ºå±äºã€Šé£é™©æ¸…å•ã€‹çš„ç‹¬ç«‹é£é™©æ¡ç›®
  "memo_length": <int>
}}
2) passed ä¸º true çš„å……è¦æ¡ä»¶ï¼šrubric.required_sections å…¨éƒ¨å‡ºç°ï¼›risk_items æ•°é‡ >= rubric.min_risksï¼›memo_length <= rubric.max_words
3) issues å¿…é¡»å…·ä½“ã€å¯æ“ä½œï¼ˆä¾‹å¦‚â€œé£é™©æ¡ç›®ä¸è¶³ï¼šç›®å‰3æ¡ï¼Œéœ€è¦>=5æ¡â€ï¼‰

ã€rubricã€‘
{rubric_json}

ã€memoã€‘
{memo}
"""
        resp = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )

        passed = False
        score = 10.0
        issues: List[str] = ["judge_parse_failed"]
        risk_items: List[str] = []

        txt = resp.choices[0].message.content or ""
        obj = _json_extract(txt) or {}
        try:
            if isinstance(obj, dict):
                passed = bool(obj.get("passed"))
                score = float(obj.get("score", 0.0) or 0.0)
                issues_raw = obj.get("issues") or []
                if isinstance(issues_raw, list):
                    issues = [str(x) for x in issues_raw if str(x).strip()]
                risk_raw = obj.get("risk_items") or []
                if isinstance(risk_raw, list):
                    risk_items = [str(x) for x in risk_raw if str(x).strip()]
        except Exception:
            pass

        memo_len = len(memo or "")
        # åŒä¿é™©ï¼šç”¨ rubric å†æ ¡éªŒä¸€æ¬¡ï¼ˆé¿å…æ¨¡å‹æ¼å†™ï¼‰
        if memo_len > 0:
            if max_words > 0 and memo_len > max_words and all("å†…å®¹è¿‡é•¿" not in str(x) for x in issues):
                issues.append(f"å†…å®¹è¿‡é•¿: {memo_len} > {max_words}")
            if min_risks > 0 and len(risk_items) < min_risks and all("é£é™©æ¡æ•°ä¸è¶³" not in str(x) for x in issues):
                issues.append(f"é£é™©æ¡æ•°ä¸è¶³: {len(risk_items)} < {min_risks}")
            passed = (len(issues) == 0)

        verification = {
            "passed": passed,
            "issues": issues,
            "memo_length": memo_len,
            "min_risks": min_risks,
            "max_words": max_words,
            "risk_items_count": len(risk_items),
            "risk_count_method": "llm_only",
            "risk_items_preview": risk_items[:10],
        }

        self.state["rubric"] = rubric
        self.state["verification"] = verification

        print(
            f"   âœ… éªŒè¯å®Œæˆ: passed={passed} score={score} issues={len(issues)} risk_items={len(risk_items)}"
        )
        return {
            "success": True,
            "passed": passed,
            "issues": issues,
            "score": score,
            "verification": verification,
        }

    async def _reflect_feedback_tool(self, args: Dict[str, Any]) -> Dict[str, Any]:
        requirements = (args.get("requirements") or self.state.get("requirements") or "").strip()
        memo = (args.get("memo") or self.state.get("memo") or "").strip()
        verification = args.get("verification") or self.state.get("verification") or {}
        rubric = args.get("rubric") or self.state.get("rubric") or {}

        if not requirements or not memo:
            return {"success": False, "error": "missing_requirements_or_memo"}

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªâ€œå¤‡å¿˜å½•è´¨é‡æ”¹è¿›åŠ©æ‰‹â€ã€‚è¯·æ ¹æ®éªŒè¯æŠ¥å‘Šï¼Œè¾“å‡ºä¸‹ä¸€è½®ç”Ÿæˆ memo çš„â€œå¯æ‰§è¡Œæ”¹è¿›æŒ‡ä»¤â€ã€‚

ã€requirementsã€‘
{requirements}

ã€rubricã€‘
{json.dumps(rubric, ensure_ascii=False, indent=2)}

ã€verification æŠ¥å‘Šã€‘
{json.dumps(verification, ensure_ascii=False, indent=2)}

ã€ä¸Šä¸€ç‰ˆ memoï¼ˆæˆªæ–­ï¼‰ã€‘
{memo[:1800]}

è¾“å‡ºè¦æ±‚ï¼š
1) åªè¾“å‡ºæ”¹è¿›æŒ‡ä»¤æ–‡æœ¬ï¼ˆä¸è¦ JSONï¼‰
2) å¿…é¡»é€æ¡å¯¹åº” verification.issuesï¼Œç»™å‡ºæ˜ç¡®çš„â€œåº”è¯¥è¡¥ä»€ä¹ˆ/åˆ ä»€ä¹ˆ/æ”¹ä»€ä¹ˆâ€
3) å¦‚æœå­—æ•°è¶…é™ï¼Œç»™å‡ºç²¾ç®€ç­–ç•¥ï¼ˆåˆ å†—ä½™ã€åˆå¹¶æ®µè½ã€æ”¹ä¸ºåˆ—è¡¨ï¼‰
4) ç»™å‡ºä¸€ä¸ªâ€œä¼˜å…ˆçº§é¡ºåºâ€ï¼ˆå…ˆä¿®æœ€å½±å“é€šè¿‡éªŒè¯çš„ç‚¹ï¼‰
"""
        resp = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        feedback = (resp.choices[0].message.content or "").strip()

        self.state["feedback"] = feedback
        print(f"   âœ… åæ€å®Œæˆ: feedback_len={len(feedback)}")
        return {"success": True, "feedback": feedback, "rubric": rubric}


# ==================== Agent ä¸»å¾ªç¯ï¼ˆä»¿ç…§ main.pyï¼‰ ====================


async def run_openai_agent(requirements: str) -> Dict[str, Any]:
    """è¿è¡Œ OpenAI Function Calling Agentï¼ˆmemo ç‰ˆæœ¬ï¼‰"""

    print("=" * 70)
    print("ğŸ“ OpenAI Function Calling Agent (Adaptive Memo)")
    print("=" * 70)

    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")
    model = os.getenv("OPENAI_MODEL", "deepseek-chat")

    client = AsyncOpenAI(api_key=api_key, base_url=base_url)
    tool_executor = ToolExecutor(client, model)
    tracker = TokenTracker()

    print(f"\nâœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (model: {model})")

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªâ€œå¤‡å¿˜å½•ç”Ÿæˆæ™ºèƒ½ä½“â€(tools-call)ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·å®Œæˆä»»åŠ¡ï¼š
1) generate_memo_tool - ç”Ÿæˆ memoï¼ˆrubric ç¼ºå¤±æ—¶å·¥å…·ä¼š llm_infer è¡¥å…¨ï¼‰
2) verify_memo_tool - LLM judge ä¸¥æ ¼éªŒè¯ memoï¼ˆè¿”å› passed/issuesï¼‰
3) reflect_feedback_tool - æ ¹æ®éªŒè¯æŠ¥å‘Šç”Ÿæˆä¸‹ä¸€è½®å¯æ‰§è¡Œæ”¹è¿›æŒ‡ä»¤

è§„åˆ™ï¼š
1) å¿…é¡»é€šè¿‡è°ƒç”¨å·¥å…·æ¥è¿­ä»£ï¼šgenerate -> verify -> (reflect -> generate -> verify ...) ç›´åˆ° passed=true æˆ–è¾¾åˆ°è¿­ä»£ä¸Šé™ã€‚
2) ä¸è¦ç›´æ¥åœ¨ message.content è¾“å‡º memo æ­£æ–‡ï¼›memo å¿…é¡»é€šè¿‡ generate_memo_tool äº§å‡ºã€‚
3) æ¯æ¬¡å·¥å…·è°ƒç”¨å°½é‡æºå¸¦æœ€æ–° memo/rubric/verification/feedbackï¼ˆä½†å³ä½¿ç¼ºå¤±ï¼Œè„šæœ¬ä¼šç”¨ state å…œåº•ï¼‰ã€‚
"""

    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": requirements},
    ]

    print(f"\nğŸ“‹ ç”¨æˆ·éœ€æ±‚:\n{requirements.strip()}")
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ...")
    print("=" * 70)

    start_time = time.time()
    max_iterations = 15
    iteration = 0

    # ä¸ºäº†ä¿è¯â€œä¸ä¼šæ— å£°ç»“æŸâ€ï¼Œå¦‚æœæ¨¡å‹è¿ç»­ä¸è°ƒå·¥å…·ï¼Œå°±å¼ºåˆ¶æç¤ºå¹¶æœ€ç»ˆå…œåº•è¾“å‡º best memo
    non_tool_rounds = 0
    max_non_tool_rounds = 3

    best_candidate = {"memo": "", "score": -1.0, "issues": []}
    final_output = ""

    try:
        while iteration < max_iterations:
            iteration += 1
            print(f"\n--- è¿­ä»£ {iteration} ---")

            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                tools=TOOLS_SCHEMA,
                tool_choice="auto",
                temperature=0.7,
            )

            if response.usage:
                tracker.add(response.usage.total_tokens, f"iteration_{iteration}")

            message = response.choices[0].message

            # æœ‰å·¥å…·è°ƒç”¨ï¼šæ‰§è¡Œå·¥å…·
            if message.tool_calls:
                non_tool_rounds = 0
                messages.append(message)

                for tool_call in message.tool_calls:
                    func_name = tool_call.function.name
                    try:
                        func_args = json.loads(tool_call.function.arguments or "{}")
                    except Exception:
                        func_args = {}

                    result = await tool_executor.execute(func_name, func_args)

                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": result[:5000],
                        }
                    )

                    # é¢å¤– tracingï¼šreplanï¼ˆverify ä¸é€šè¿‡æ—¶ï¼‰
                    if func_name == "verify_memo_tool":
                        obj = _json_extract(result) or {}
                        passed = bool(obj.get("passed"))
                        score = float(obj.get("score", 0.0) or 0.0)
                        issues = obj.get("issues") or []
                        if isinstance(issues, list):
                            issues = [str(x) for x in issues if str(x).strip()]
                        else:
                            issues = []

                        # best memo å…œåº•
                        memo_now = tool_executor.state.get("memo") or ""
                        if memo_now and score > float(best_candidate.get("score", -1.0) or -1.0):
                            best_candidate = {"memo": memo_now, "score": score, "issues": issues}

                        if not passed:
                            first_issue = issues[0] if issues else "unknown"
                            print(f"   âš ï¸ å‘ç”Ÿ replan: {first_issue}")
                            # å¼ºæç¤ºä¸‹ä¸€æ­¥èµ° reflect->generate->verifyï¼Œé¿å…æ¨¡å‹â€œç»“æŸâ€æˆ–ä¹±è·³
                            messages.append(
                                {
                                    "role": "system",
                                    "content": "éªŒè¯æœªé€šè¿‡ï¼šè¯·è°ƒç”¨ reflect_feedback_tool ç”Ÿæˆæ”¹è¿›æŒ‡ä»¤ï¼Œç„¶åè°ƒç”¨ generate_memo_tool äº§å‡ºä¿®è®¢ç‰ˆï¼Œå†æ¬¡ verifyã€‚",
                                }
                            )

                # æˆåŠŸæ¡ä»¶ï¼špassed=true ç›´æ¥æ”¶æ•›
                verification = tool_executor.state.get("verification") or {}
                if isinstance(verification, dict) and verification.get("passed") is True:
                    final_output = tool_executor.state.get("memo") or ""
                    print("\nâœ… å·²é€šè¿‡éªŒè¯ï¼Œç»“æŸè¿­ä»£")
                    break

            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼šmemo demo é‡Œé€šå¸¸æ˜¯ä¸åˆè§„ï¼ˆå®¹æ˜“â€œä¸€æ¬¡å°±ç»“æŸâ€ï¼‰
                non_tool_rounds += 1
                messages.append({"role": "assistant", "content": message.content or ""})
                messages.append(
                    {
                        "role": "system",
                        "content": "ä½ å¿…é¡»è°ƒç”¨å·¥å…·ï¼ˆgenerate/verify/reflectï¼‰ï¼Œä¸è¦ç›´æ¥è¾“å‡ºå†…å®¹ã€‚è‹¥æœªé€šè¿‡éªŒè¯ï¼Œä¸å…è®¸ç»“æŸã€‚",
                    }
                )
                print("   âš ï¸ æ¨¡å‹æœªè°ƒç”¨å·¥å…·ï¼Œå·²å¼ºåˆ¶æç¤ºç»§ç»­ä½¿ç”¨å·¥å…·")

                if non_tool_rounds >= max_non_tool_rounds:
                    print("   âš ï¸ è¿ç»­å¤šè½®æœªè°ƒç”¨å·¥å…·ï¼Œè§¦å‘å…œåº•è¾“å‡º")
                    break

        end_time = time.time()
        duration_ms = (end_time - start_time) * 1000

        # æœ€ç»ˆè¾“å‡ºï¼šä¼˜å…ˆç”¨é€šè¿‡éªŒè¯çš„ memoï¼Œå¦åˆ™ç”¨ best_candidate
        if not final_output:
            final_output = tool_executor.state.get("memo") or best_candidate.get("memo") or ""

        appendix = ""
        verification = tool_executor.state.get("verification") or {}
        passed = isinstance(verification, dict) and verification.get("passed") is True
        if not passed:
            issues = best_candidate.get("issues") or (verification.get("issues") if isinstance(verification, dict) else []) or []
            if isinstance(issues, list) and issues:
                appendix = "\n\né™„æ³¨ï¼ˆæœªé€šè¿‡éªŒè¯ï¼Œå‰©ä½™é—®é¢˜ï¼‰ï¼š\n" + "\n".join([f"- {x}" for x in issues[:10]])

        output_text = (final_output + appendix).strip() or "ï¼ˆæœªç”Ÿæˆæœ‰æ•ˆ memoï¼‰"

        print("\n" + "=" * 70)
        print("âœ… æ‰§è¡Œå®Œæˆ!")
        print("=" * 70)
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
        print(f"   - è¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"   - LLM è°ƒç”¨(å†³ç­–è½®): {tracker.llm_call_count}")
        print(f"   - Token æ¶ˆè€—(å†³ç­–è½®): {tracker.cumulative_tokens:,}")
        print(f"   - è€—æ—¶: {duration_ms:.1f}ms")

        print(f"\nğŸ“Š Token æ¶ˆè€—æ˜ç»†:")
        for step in tracker.steps:
            print(
                f"      {step['step']}: +{step['tokens']:,} (ç´¯è®¡: {step['cumulative']:,})"
            )

        print("\n" + "=" * 70)
        print("ğŸ“„ æœ€ç»ˆè¾“å‡ºï¼ˆOpenAI å®˜æ–¹ client + tools callï¼Œ3 tools è‡ªä¸»è§„åˆ’ï¼‰")
        print("=" * 70)
        print(output_text)

        return {
            "success": True,
            "output": output_text,
            "iterations": iteration,
            "llm_calls": tracker.llm_call_count,
            "total_tokens": tracker.cumulative_tokens,
            "token_steps": tracker.steps,
            "duration_ms": duration_ms,
            "passed": passed,
        }

    except Exception as e:
        end_time = time.time()
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "duration_ms": (end_time - start_time) * 1000,
            "total_tokens": tracker.cumulative_tokens,
        }


async def main():
    """ä¸»å‡½æ•°ï¼ˆä»¿ç…§ main.pyï¼‰"""
    print("ğŸš€ å¯åŠ¨ OpenAI Function Calling Adaptive Memo Demo...")

    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® API Keyï¼ˆOPENAI_API_KEY æˆ– DEEPSEEK_API_KEYï¼‰")
        return

    requirements = """
è¯·ä¸ºå›¢é˜Ÿå†™ä¸€ä»½â€œæŠ€æœ¯å†³ç­–å¤‡å¿˜å½•â€ï¼Œä¸»é¢˜æ˜¯ï¼šæˆ‘ä»¬æ˜¯å¦è¦æŠŠç°æœ‰å•ä½“æœåŠ¡æ‹†åˆ†ä¸ºå¾®æœåŠ¡ã€‚

éœ€æ±‚æè¿°:
1) èƒŒæ™¯ï¼šç›®å‰å•ä½“å·²å‡ºç°éƒ¨ç½²é¢‘ç‡ä½ã€å‘å¸ƒé£é™©å¤§ã€éƒ¨åˆ†æ¨¡å—æ€§èƒ½ç“¶é¢ˆ
2) ç›®æ ‡ï¼šåœ¨ä¸ç‰ºç‰²äº¤ä»˜é€Ÿåº¦çš„å‰æä¸‹ï¼Œæé«˜å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’Œæ•…éšœéš”ç¦»èƒ½åŠ›
3) çº¦æŸï¼šå›¢é˜Ÿäººæ•° 6 äººï¼›æœªæ¥ 3 ä¸ªæœˆä¸»è¦ç›®æ ‡æ˜¯ç¨³å®šäº¤ä»˜ï¼›è¿ç»´èƒ½åŠ›ä¸€èˆ¬ï¼›é¢„ç®—æœ‰é™
4) è¾“å‡ºå¿…é¡»åŒ…å«ï¼šèƒŒæ™¯ã€å†³ç­–ã€æ–¹æ¡ˆå¯¹æ¯”ã€é£é™©æ¸…å•ã€ä¸‹ä¸€æ­¥

è¯·æŒ‰â€œå…ˆç»™å‡ºåˆç¨¿ -> ä¸¥æ ¼éªŒè¯ -> åæ€ä¸è¶³ -> ä¿®è®¢ -> å†éªŒè¯â€çš„æ–¹å¼è¿­ä»£åˆ°é€šè¿‡éªŒè¯ã€‚
""".strip()

    result = await run_openai_agent(requirements)

    # ä¿å­˜ç»“æœï¼ˆä»¿ç…§ main.pyï¼‰
    if result.get("success"):
        output_dir = script_dir / "output"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"openai_fc_adaptive_memo_{timestamp}.md"

        token_detail = "\n### Token æ¶ˆè€—æ˜ç»†ï¼ˆä»…å†³ç­–è½®ï¼‰\n\n| æ­¥éª¤ | Token | ç´¯è®¡ |\n|------|-------|------|\n"
        for step in result.get("token_steps", []):
            token_detail += (
                f"| {step['step']} | {step['tokens']:,} | {step['cumulative']:,} |\n"
            )

        output_file.write_text(
            f"""# OpenAI Function Calling - Adaptive Memo

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
> æ¡†æ¶: OpenAI Native Function Calling
> é€šè¿‡éªŒè¯: {result.get("passed")}

---

{result.get("output", "")}

---

## æ‰§è¡Œç»Ÿè®¡

- è¿­ä»£æ¬¡æ•°: {result.get("iterations")}
- LLM è°ƒç”¨æ¬¡æ•°(å†³ç­–è½®): {result.get("llm_calls")}
- Token æ¶ˆè€—(å†³ç­–è½®): {result.get("total_tokens", 0):,}
- è€—æ—¶: {result.get("duration_ms", 0.0):.1f}ms
{token_detail}
""",
            encoding="utf-8",
        )

        print(f"\nğŸ“„ è¾“å‡ºå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    print("=" * 70)
    print("OpenAI Function Calling Adaptive Memo Demo")
    print("=" * 70)
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")

