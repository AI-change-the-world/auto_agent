"""
è·¨æ­¥éª¤æ™ºèƒ½è§„åˆ’ Demo - æµ‹è¯• Replan ä¼˜åŒ–åŠŸèƒ½

æ¼”ç¤ºæ–°å®ç°çš„åŠŸèƒ½ï¼š
1. ä»»åŠ¡å¤æ‚åº¦åˆ†çº§ (TaskComplexity)
2. æ‰§è¡Œç­–ç•¥é€‰æ‹© (ExecutionStrategy)
3. è·¨æ­¥éª¤å·¥ä½œè®°å¿† (CrossStepWorkingMemory)
4. å…¨å±€ä¸€è‡´æ€§æ£€æŸ¥ (GlobalConsistencyChecker)
5. å¢é‡å¼é‡è§„åˆ’ (_incremental_replan)

åœºæ™¯ï¼šæ¨¡æ‹Ÿä¸€ä¸ª"API æœåŠ¡é¡¹ç›®ç”Ÿæˆ"ä»»åŠ¡
- è®¾è®¡ API æ¥å£ â†’ ç”Ÿæˆæ•°æ®æ¨¡å‹ â†’ å®ç°ä¸šåŠ¡é€»è¾‘ â†’ ç”Ÿæˆæµ‹è¯•ä»£ç 
- æ¯ä¸ªæ­¥éª¤éƒ½ä¼šäº§ç”Ÿçº¦æŸï¼Œåç»­æ­¥éª¤å¿…é¡»éµå®ˆ
- æ•…æ„åœ¨æŸäº›æ­¥éª¤å¼•å…¥ä¸ä¸€è‡´ï¼Œæµ‹è¯•ä¸€è‡´æ€§æ£€æŸ¥

ä½¿ç”¨æ–¹æ³•:
    export OPENAI_API_KEY=your-key  # æˆ– DEEPSEEK_API_KEY
    python examples/cross_step_replan_demo.py
"""

import asyncio
import json
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° pathï¼Œç¡®ä¿ä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from auto_agent import (
    AutoAgent,
    BaseTool,
    OpenAIClient,
    ToolDefinition,
    ToolParameter,
    ToolRegistry,
)
from auto_agent.models import ToolReplanPolicy

# ==================== LLM å®¢æˆ·ç«¯é…ç½® ====================


def get_llm_client() -> Optional[OpenAIClient]:
    """è·å– LLM å®¢æˆ·ç«¯"""
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        return None

    return OpenAIClient(
        api_key=api_key,
        base_url=os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1"),
        model=os.getenv("OPENAI_MODEL", "deepseek-chat"),
        timeout=120.0,
    )


# ==================== å·¥å…·å®šä¹‰ï¼ˆå¸¦ replan_policyï¼‰====================


class DesignAPITool(BaseTool):
    """
    API æ¥å£è®¾è®¡å·¥å…·

    è¿™æ˜¯ä¸€ä¸ªé«˜å½±å“åŠ›å·¥å…·ï¼Œä¼šäº§ç”Ÿåç»­æ­¥éª¤å¿…é¡»éµå®ˆçš„æ¥å£çº¦æŸ
    """

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="design_api",
            description="è®¾è®¡ API æ¥å£ï¼Œå®šä¹‰ç«¯ç‚¹ã€è¯·æ±‚/å“åº”æ ¼å¼ã€‚è¿™æ˜¯é¡¹ç›®çš„ç¬¬ä¸€æ­¥ï¼Œä¼šäº§ç”Ÿåç»­å¿…é¡»éµå®ˆçš„æ¥å£çº¦æŸã€‚",
            parameters=[
                ToolParameter(
                    name="project_name",
                    type="string",
                    description="é¡¹ç›®åç§°",
                    required=True,
                ),
                ToolParameter(
                    name="requirements",
                    type="string",
                    description="åŠŸèƒ½éœ€æ±‚æè¿°",
                    required=True,
                ),
            ],
            category="design",
            output_schema={
                "api_design": {"type": "object", "description": "API è®¾è®¡ç»“æœ"},
                "endpoints": {"type": "array", "description": "ç«¯ç‚¹åˆ—è¡¨"},
                "constraints": {"type": "array", "description": "æ¥å£çº¦æŸ"},
            },
            # é«˜å½±å“åŠ›å·¥å…·ï¼Œéœ€è¦ä¸€è‡´æ€§æ£€æŸ¥
            replan_policy=ToolReplanPolicy(
                high_impact=True,
                requires_consistency_check=True,
                force_replan_check=False,
            ),
        )

    async def execute(
        self,
        project_name: str,
        requirements: str,
        **kwargs,
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM è®¾è®¡ API æ¥å£"""
        prompt = f"""è¯·ä¸ºä»¥ä¸‹é¡¹ç›®è®¾è®¡ RESTful API æ¥å£ã€‚

é¡¹ç›®åç§°: {project_name}
åŠŸèƒ½éœ€æ±‚: {requirements}

è¯·è¿”å› JSON æ ¼å¼çš„è®¾è®¡ç»“æœï¼š
```json
{{
    "project_name": "{project_name}",
    "endpoints": [
        {{
            "method": "GET/POST/PUT/DELETE",
            "path": "/api/xxx",
            "description": "æ¥å£æè¿°",
            "request_params": {{"param_name": "type"}},
            "response_schema": {{"field": "type"}}
        }}
    ],
    "data_models": [
        {{
            "name": "ModelName",
            "fields": {{"field_name": "type"}}
        }}
    ],
    "constraints": [
        "æ‰€æœ‰ ID å­—æ®µå¿…é¡»ä½¿ç”¨æ•´æ•°ç±»å‹",
        "æ—¶é—´å­—æ®µä½¿ç”¨ ISO 8601 æ ¼å¼"
    ]
}}
```"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}],
                temperature=0.4,
                max_tokens=2000,
            )

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"success": False, "error": "æ— æ³•è§£æ API è®¾è®¡ç»“æœ"}

            result["success"] = True
            result["api_design"] = {
                "endpoints": result.get("endpoints", []),
                "data_models": result.get("data_models", []),
            }
            return result

        except Exception as e:
            return {"success": False, "error": str(e)}


class GenerateModelTool(BaseTool):
    """
    æ•°æ®æ¨¡å‹ç”Ÿæˆå·¥å…·

    å¿…é¡»åŸºäº API è®¾è®¡ä¸­å®šä¹‰çš„æ•°æ®æ¨¡å‹ï¼Œä¿æŒä¸€è‡´æ€§
    """

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="generate_model",
            description="æ ¹æ® API è®¾è®¡ç”Ÿæˆæ•°æ®æ¨¡å‹ä»£ç ã€‚å¿…é¡»ä¸ API è®¾è®¡ä¸­çš„æ•°æ®æ¨¡å‹å®šä¹‰ä¿æŒä¸€è‡´ã€‚",
            parameters=[
                ToolParameter(
                    name="api_design",
                    type="object",
                    description="API è®¾è®¡ç»“æœï¼ˆä» design_api è·å–ï¼‰",
                    required=True,
                ),
                ToolParameter(
                    name="language",
                    type="string",
                    description="ç¼–ç¨‹è¯­è¨€: python/typescript/java",
                    required=False,
                ),
            ],
            category="code_generation",
            output_schema={
                "model_code": {"type": "string", "description": "ç”Ÿæˆçš„æ¨¡å‹ä»£ç "},
                "model_definitions": {"type": "object", "description": "æ¨¡å‹å®šä¹‰"},
            },
            param_aliases={
                "api_design": "api_design",
            },
            replan_policy=ToolReplanPolicy(
                high_impact=True,
                requires_consistency_check=True,
                consistency_check_against=["interface"],
            ),
        )

    async def execute(
        self,
        api_design: Dict[str, Any],
        language: str = "python",
        **kwargs,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ¨¡å‹ä»£ç """
        data_models = api_design.get("data_models", [])

        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®æ¨¡å‹å®šä¹‰ç”Ÿæˆ {language} ä»£ç ã€‚

æ•°æ®æ¨¡å‹å®šä¹‰:
{json.dumps(data_models, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. ä½¿ç”¨ dataclassï¼ˆPythonï¼‰æˆ– interfaceï¼ˆTypeScriptï¼‰
2. æ·»åŠ ç±»å‹æ³¨è§£
3. æ·»åŠ æ–‡æ¡£æ³¨é‡Š
4. å­—æ®µç±»å‹å¿…é¡»ä¸å®šä¹‰å®Œå…¨ä¸€è‡´

è¯·è¿”å› JSON æ ¼å¼ï¼š
```json
{{
    "language": "{language}",
    "model_code": "å®Œæ•´çš„æ¨¡å‹ä»£ç ",
    "model_definitions": {{
        "ModelName": {{
            "fields": {{"field": "type"}},
            "methods": []
        }}
    }}
}}
```"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000,
            )

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"success": False, "error": "æ— æ³•è§£ææ¨¡å‹ä»£ç "}

            result["success"] = True
            return result

        except Exception as e:
            return {"success": False, "error": str(e)}


class ImplementServiceTool(BaseTool):
    """
    ä¸šåŠ¡é€»è¾‘å®ç°å·¥å…·

    å¿…é¡»ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„æ•°æ®æ¨¡å‹å’Œ API æ¥å£
    """

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="implement_service",
            description="å®ç°ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚å¿…é¡»ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„æ•°æ®æ¨¡å‹ï¼Œå®ç° API è®¾è®¡ä¸­çš„ç«¯ç‚¹ã€‚",
            parameters=[
                ToolParameter(
                    name="api_design",
                    type="object",
                    description="API è®¾è®¡ç»“æœ",
                    required=True,
                ),
                ToolParameter(
                    name="model_definitions",
                    type="object",
                    description="æ¨¡å‹å®šä¹‰ï¼ˆä» generate_model è·å–ï¼‰",
                    required=True,
                ),
            ],
            category="code_generation",
            output_schema={
                "service_code": {"type": "string", "description": "æœåŠ¡ä»£ç "},
                "implemented_endpoints": {
                    "type": "array",
                    "description": "å·²å®ç°çš„ç«¯ç‚¹",
                },
            },
            param_aliases={
                "api_design": "api_design",
                "model_definitions": "model_definitions",
            },
            replan_policy=ToolReplanPolicy(
                high_impact=True,
                requires_consistency_check=True,
                replan_condition="å¦‚æœå®ç°çš„æ¥å£ä¸è®¾è®¡ä¸ä¸€è‡´",
            ),
        )

    async def execute(
        self,
        api_design: Dict[str, Any],
        model_definitions: Dict[str, Any],
        **kwargs,
    ) -> Dict[str, Any]:
        """å®ç°ä¸šåŠ¡é€»è¾‘"""
        endpoints = api_design.get("endpoints", [])

        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ API è®¾è®¡å’Œæ•°æ®æ¨¡å‹å®ç°ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚

API ç«¯ç‚¹:
{json.dumps(endpoints, ensure_ascii=False, indent=2)}

æ•°æ®æ¨¡å‹:
{json.dumps(model_definitions, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. ä¸ºæ¯ä¸ªç«¯ç‚¹å®ç°å¯¹åº”çš„å¤„ç†å‡½æ•°
2. ä½¿ç”¨å®šä¹‰çš„æ•°æ®æ¨¡å‹
3. æ·»åŠ åŸºæœ¬çš„é”™è¯¯å¤„ç†
4. å‡½æ•°ç­¾åå¿…é¡»ä¸ API è®¾è®¡ä¸€è‡´

è¯·è¿”å› JSON æ ¼å¼ï¼š
```json
{{
    "service_code": "å®Œæ•´çš„æœåŠ¡ä»£ç ",
    "implemented_endpoints": [
        {{
            "path": "/api/xxx",
            "method": "GET",
            "function_name": "get_xxx"
        }}
    ]
}}
```"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=3000,
            )

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"success": False, "error": "æ— æ³•è§£ææœåŠ¡ä»£ç "}

            result["success"] = True
            return result

        except Exception as e:
            return {"success": False, "error": str(e)}


class GenerateTestTool(BaseTool):
    """
    æµ‹è¯•ä»£ç ç”Ÿæˆå·¥å…·

    å¿…é¡»è¦†ç›–æ‰€æœ‰å·²å®ç°çš„ç«¯ç‚¹
    """

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="generate_tests",
            description="ç”Ÿæˆæµ‹è¯•ä»£ç ï¼Œè¦†ç›–æ‰€æœ‰å·²å®ç°çš„ API ç«¯ç‚¹ã€‚",
            parameters=[
                ToolParameter(
                    name="implemented_endpoints",
                    type="array",
                    description="å·²å®ç°çš„ç«¯ç‚¹åˆ—è¡¨",
                    required=True,
                ),
                ToolParameter(
                    name="model_definitions",
                    type="object",
                    description="æ¨¡å‹å®šä¹‰",
                    required=True,
                ),
            ],
            category="testing",
            output_schema={
                "test_code": {"type": "string", "description": "æµ‹è¯•ä»£ç "},
                "test_coverage": {"type": "object", "description": "æµ‹è¯•è¦†ç›–æƒ…å†µ"},
            },
            param_aliases={
                "implemented_endpoints": "implemented_endpoints",
                "model_definitions": "model_definitions",
            },
        )

    async def execute(
        self,
        implemented_endpoints: List[Dict[str, Any]],
        model_definitions: Dict[str, Any],
        **kwargs,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•ä»£ç """
        prompt = f"""è¯·ä¸ºä»¥ä¸‹ API ç«¯ç‚¹ç”Ÿæˆæµ‹è¯•ä»£ç ã€‚

å·²å®ç°çš„ç«¯ç‚¹:
{json.dumps(implemented_endpoints, ensure_ascii=False, indent=2)}

æ•°æ®æ¨¡å‹:
{json.dumps(model_definitions, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. ä½¿ç”¨ pytest æ¡†æ¶
2. ä¸ºæ¯ä¸ªç«¯ç‚¹è‡³å°‘ç”Ÿæˆ 2 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆæ­£å¸¸å’Œå¼‚å¸¸ï¼‰
3. ä½¿ç”¨ mock æ•°æ®

è¯·è¿”å› JSON æ ¼å¼ï¼š
```json
{{
    "test_code": "å®Œæ•´çš„æµ‹è¯•ä»£ç ",
    "test_coverage": {{
        "total_endpoints": 0,
        "covered_endpoints": 0,
        "test_cases": []
    }}
}}
```"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=8192,
            )

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"success": False, "error": "æ— æ³•è§£ææµ‹è¯•ä»£ç "}

            result["success"] = True
            return result

        except Exception as e:
            return {"success": False, "error": str(e)}


class ReviewCodeTool(BaseTool):
    """
    ä»£ç å®¡æŸ¥å·¥å…·

    æ£€æŸ¥ä»£ç ä¸€è‡´æ€§å’Œè´¨é‡
    """

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="review_code",
            description="å®¡æŸ¥ç”Ÿæˆçš„ä»£ç ï¼Œæ£€æŸ¥ä¸€è‡´æ€§ã€è´¨é‡å’Œæ½œåœ¨é—®é¢˜ã€‚",
            parameters=[
                ToolParameter(
                    name="api_design",
                    type="object",
                    description="API è®¾è®¡",
                    required=True,
                ),
                ToolParameter(
                    name="model_code",
                    type="string",
                    description="æ¨¡å‹ä»£ç ",
                    required=True,
                ),
                ToolParameter(
                    name="service_code",
                    type="string",
                    description="æœåŠ¡ä»£ç ",
                    required=True,
                ),
            ],
            category="review",
            output_schema={
                "review_result": {"type": "object", "description": "å®¡æŸ¥ç»“æœ"},
                "issues": {"type": "array", "description": "å‘ç°çš„é—®é¢˜"},
                "suggestions": {"type": "array", "description": "æ”¹è¿›å»ºè®®"},
            },
            param_aliases={
                "api_design": "api_design",
                "model_code": "model_code",
                "service_code": "service_code",
            },
        )

    async def execute(
        self,
        api_design: Dict[str, Any],
        model_code: str,
        service_code: str,
        **kwargs,
    ) -> Dict[str, Any]:
        """å®¡æŸ¥ä»£ç """
        prompt = f"""è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼Œæ£€æŸ¥ä¸€è‡´æ€§å’Œè´¨é‡ã€‚

API è®¾è®¡:
{json.dumps(api_design, ensure_ascii=False, indent=2)[:1500]}

æ¨¡å‹ä»£ç :
{model_code[:2000]}

æœåŠ¡ä»£ç :
{service_code[:2000]}

è¯·æ£€æŸ¥:
1. æ¨¡å‹æ˜¯å¦ä¸ API è®¾è®¡ä¸€è‡´
2. æœåŠ¡æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†æ¨¡å‹
3. æ¥å£å®ç°æ˜¯å¦å®Œæ•´
4. ä»£ç è´¨é‡é—®é¢˜

è¯·è¿”å› JSON æ ¼å¼ï¼š
```json
{{
    "consistency_score": 0.0-1.0,
    "quality_score": 0.0-1.0,
    "issues": [
        {{"type": "consistency/quality/security", "description": "é—®é¢˜æè¿°", "severity": "high/medium/low"}}
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
    "summary": "å®¡æŸ¥æ€»ç»“"
}}
```"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}],
                temperature=0.4,
                max_tokens=2000,
            )

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"success": False, "error": "æ— æ³•è§£æå®¡æŸ¥ç»“æœ"}

            result["success"] = True
            result["review_result"] = {
                "consistency_score": result.get("consistency_score", 0),
                "quality_score": result.get("quality_score", 0),
                "summary": result.get("summary", ""),
            }
            return result

        except Exception as e:
            return {"success": False, "error": str(e)}


# ==================== ä¸»ç¨‹åº ====================


async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•è·¨æ­¥éª¤æ™ºèƒ½è§„åˆ’åŠŸèƒ½"""
    print("=" * 70)
    print("ğŸ”§ è·¨æ­¥éª¤æ™ºèƒ½è§„åˆ’ Demo - æµ‹è¯• Replan ä¼˜åŒ–åŠŸèƒ½")
    print("=" * 70)

    # 1. è·å– LLM å®¢æˆ·ç«¯
    llm_client = get_llm_client()
    if not llm_client:
        print("\nâŒ æœªè®¾ç½® API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   export OPENAI_API_KEY=your-api-key")
        return

    print("\nâœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")

    # 2. æ³¨å†Œå·¥å…·
    print("\nğŸ”§ æ³¨å†Œå·¥å…·...")
    registry = ToolRegistry()

    registry.register(DesignAPITool(llm_client))
    registry.register(GenerateModelTool(llm_client))
    registry.register(ImplementServiceTool(llm_client))
    registry.register(GenerateTestTool(llm_client))
    registry.register(ReviewCodeTool(llm_client))

    print(f"   å·²æ³¨å†Œ {len(registry.get_all_tools())} ä¸ªå·¥å…·:")
    for tool in registry.get_all_tools():
        policy = tool.definition.replan_policy
        policy_info = ""
        if policy:
            if policy.high_impact:
                policy_info = " [é«˜å½±å“åŠ›]"
            if policy.requires_consistency_check:
                policy_info += " [éœ€ä¸€è‡´æ€§æ£€æŸ¥]"
        print(f"   - {tool.definition.name}{policy_info}")

    # 3. åˆ›å»ºæ™ºèƒ½ä½“
    print("\nğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“...")
    agent = AutoAgent(
        llm_client=llm_client,
        tool_registry=registry,
        agent_name="API Project Generator",
        agent_description="ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»è§„åˆ’å’Œç”Ÿæˆ API é¡¹ç›®ä»£ç çš„æ™ºèƒ½ä½“",
        agent_goals=[
            "è®¾è®¡æ¸…æ™°çš„ API æ¥å£",
            "ç”Ÿæˆä¸€è‡´çš„æ•°æ®æ¨¡å‹",
            "å®ç°å®Œæ•´çš„ä¸šåŠ¡é€»è¾‘",
            "ç”Ÿæˆæµ‹è¯•ä»£ç ",
            "ç¡®ä¿ä»£ç è´¨é‡",
        ],
        agent_constraints=[
            "åç»­æ­¥éª¤å¿…é¡»ä¸ API è®¾è®¡ä¿æŒä¸€è‡´",
            "æ•°æ®æ¨¡å‹å­—æ®µç±»å‹å¿…é¡»ä¸¥æ ¼éµå®ˆ",
            "æ‰€æœ‰ç«¯ç‚¹éƒ½å¿…é¡»æœ‰å¯¹åº”å®ç°",
        ],
    )

    # 4. ç”¨æˆ·éœ€æ±‚
    user_query = """
    è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªç®€å•çš„"ç”¨æˆ·ç®¡ç† API"é¡¹ç›®ã€‚

    åŠŸèƒ½éœ€æ±‚ï¼š
    1. ç”¨æˆ·æ³¨å†Œï¼ˆPOST /api/usersï¼‰
    2. ç”¨æˆ·ç™»å½•ï¼ˆPOST /api/auth/loginï¼‰
    3. è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆGET /api/users/{id}ï¼‰
    4. æ›´æ–°ç”¨æˆ·ä¿¡æ¯ï¼ˆPUT /api/users/{id}ï¼‰

    è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š
    1. é¦–å…ˆè®¾è®¡ API æ¥å£
    2. æ ¹æ®è®¾è®¡ç”Ÿæˆæ•°æ®æ¨¡å‹
    3. å®ç°ä¸šåŠ¡é€»è¾‘
    4. ç”Ÿæˆæµ‹è¯•ä»£ç 
    5. æœ€åå®¡æŸ¥ä»£ç è´¨é‡

    æ³¨æ„ï¼šæ¯ä¸ªæ­¥éª¤éƒ½è¦ä¸å‰é¢çš„è®¾è®¡ä¿æŒä¸€è‡´ï¼
    """

    print("\n" + "=" * 70)
    print("ğŸ“‹ ç”¨æˆ·éœ€æ±‚:")
    print("=" * 70)
    print(user_query.strip())
    print("\n" + "=" * 70)
    print("ğŸš€ æ™ºèƒ½ä½“å¼€å§‹æ‰§è¡Œ...")
    print("=" * 70)

    # 5. æ‰§è¡Œå¹¶è§‚å¯Ÿ
    execution_log = []
    final_results = {}

    try:
        async for event in agent.run_stream(
            query=user_query,
            user_id="developer",
        ):
            event_type = event.get("event")
            data = event.get("data", {})

            if event_type == "planning":
                print(f"\nğŸ“ {data.get('message', 'è§„åˆ’ä¸­...')}")

            elif event_type == "execution_plan":
                print("\n" + "-" * 50)
                print("ğŸ“‹ æ‰§è¡Œè®¡åˆ’:")
                print("-" * 50)
                for step in data.get("steps", []):
                    pinned = "ğŸ“Œ" if step.get("is_pinned") else "  "
                    print(
                        f"   {pinned} Step {step['step']}: [{step['name']}] {step['description'][:50]}..."
                    )
                print("-" * 50)

            elif event_type == "stage_start":
                step = data.get("step", "?")
                name = data.get("name", "unknown")
                desc = data.get("description", "")
                print(f"\nâ–¶ï¸  Step {step}: {name}")
                print(f"   ğŸ“ {desc[:60]}...")

            elif event_type == "stage_complete":
                step = data.get("step", "?")
                name = data.get("name", "unknown")
                success = data.get("success", False)
                result = data.get("result", {}) or {}
                status = "âœ…" if success else "âŒ"

                print(f"   {status} å®Œæˆ")

                if not success:
                    error = result.get("error", "æœªçŸ¥é”™è¯¯")
                    print(f"   â— é”™è¯¯: {error}")
                    continue

                # æ˜¾ç¤ºå…³é”®è¾“å‡º
                if isinstance(result, dict):
                    if "endpoints" in result:
                        endpoints = result.get("endpoints", [])
                        print(f"   ğŸ“¤ è®¾è®¡äº† {len(endpoints)} ä¸ªç«¯ç‚¹")
                        for ep in endpoints[:3]:
                            print(
                                f"      - {ep.get('method', '?')} {ep.get('path', '?')}"
                            )

                    if "model_code" in result:
                        code = result.get("model_code", "")
                        print(f"   ğŸ“¤ ç”Ÿæˆæ¨¡å‹ä»£ç  ({len(code)} å­—ç¬¦)")

                    if "service_code" in result:
                        code = result.get("service_code", "")
                        endpoints = result.get("implemented_endpoints", [])
                        print(
                            f"   ğŸ“¤ å®ç°äº† {len(endpoints)} ä¸ªç«¯ç‚¹ ({len(code)} å­—ç¬¦)"
                        )

                    if "test_code" in result:
                        code = result.get("test_code", "")
                        coverage = result.get("test_coverage", {})
                        print(f"   ğŸ“¤ ç”Ÿæˆæµ‹è¯•ä»£ç  ({len(code)} å­—ç¬¦)")
                        print(
                            f"      è¦†ç›–ç«¯ç‚¹: {coverage.get('covered_endpoints', 0)}/{coverage.get('total_endpoints', 0)}"
                        )

                    if "review_result" in result:
                        review = result.get("review_result", {})
                        print(f"   ğŸ“¤ å®¡æŸ¥ç»“æœ:")
                        print(f"      ä¸€è‡´æ€§: {review.get('consistency_score', 0):.0%}")
                        print(f"      è´¨é‡: {review.get('quality_score', 0):.0%}")
                        issues = result.get("issues", [])
                        if issues:
                            print(f"      å‘ç° {len(issues)} ä¸ªé—®é¢˜")

                # ä¿å­˜ç»“æœ
                final_results[name] = result
                execution_log.append(
                    {
                        "step": step,
                        "name": name,
                        "success": success,
                    }
                )

            elif event_type == "consistency_violation":
                violations = data.get("violations", [])
                print(f"\n   âš ï¸  ä¸€è‡´æ€§è¿è§„æ£€æµ‹:")
                for v in violations:
                    print(
                        f"      - [{v.get('severity', '?')}] {v.get('description', '')[:60]}..."
                    )

            elif event_type == "stage_replan":
                reason = data.get("reason", "")
                print(f"\n   ğŸ”„ è§¦å‘é‡è§„åˆ’: {reason[:60]}...")

            elif event_type == "stage_retry":
                print(f"\n   ğŸ”„ é‡è¯•: {data.get('message', '')}")

            elif event_type == "done":
                print("\n" + "=" * 70)
                success = data.get("success", False)
                iterations = data.get("iterations", 0)

                if success:
                    print(f"âœ… æ‰§è¡Œå®Œæˆ! (å…± {iterations} æ­¥)")
                else:
                    print(f"âŒ æ‰§è¡Œå¤±è´¥: {data.get('message', '')}")

                # æ˜¾ç¤ºè¿½è¸ªç»Ÿè®¡
                trace = data.get("trace")
                if trace:
                    summary = trace.get("summary", {})
                    llm_calls = summary.get("llm_calls", {})
                    print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
                    print(f"   - è¿½è¸ªID: {trace.get('trace_id', 'N/A')}")
                    print(f"   - LLMè°ƒç”¨: {llm_calls.get('count', 0)} æ¬¡")
                    print(f"   - Tokenæ¶ˆè€—: {llm_calls.get('total_tokens', 0):,}")

                print("=" * 70)

            elif event_type == "error":
                print(f"\nâŒ é”™è¯¯: {data.get('message', '')}")

        # 6. æ˜¾ç¤ºå·¥ä½œè®°å¿†å’Œä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ“‹ è·¨æ­¥éª¤æ™ºèƒ½è§„åˆ’åŠŸèƒ½éªŒè¯")
        print("=" * 70)

        # è·å–æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(agent, "_last_context") and agent._last_context:
            ctx = agent._last_context

            # å·¥ä½œè®°å¿†
            wm = ctx.working_memory
            print(f"\nğŸ§  å·¥ä½œè®°å¿†:")
            print(f"   - è®¾è®¡å†³ç­–: {len(wm.design_decisions)} æ¡")
            print(f"   - çº¦æŸæ¡ä»¶: {len(wm.constraints)} æ¡")
            print(f"   - å¾…åŠäº‹é¡¹: {len(wm.todos)} æ¡")
            print(f"   - æ¥å£å®šä¹‰: {len(wm.interfaces)} ä¸ª")

            if wm.design_decisions:
                print("\n   æœ€è¿‘çš„è®¾è®¡å†³ç­–:")
                for d in wm.design_decisions[-3:]:
                    print(f"      - {d.decision[:50]}...")

            if wm.constraints:
                print("\n   çº¦æŸæ¡ä»¶:")
                for c in wm.constraints[-3:]:
                    print(f"      - [{c.priority}] {c.constraint[:50]}...")

            # ä¸€è‡´æ€§æ£€æŸ¥å™¨
            checker = ctx.consistency_checker
            print(f"\nğŸ” ä¸€è‡´æ€§æ£€æŸ¥:")
            print(f"   - æ£€æŸ¥ç‚¹: {len(checker.checkpoints)} ä¸ª")
            print(f"   - è¿è§„è®°å½•: {len(checker.violations)} æ¡")

            if checker.checkpoints:
                print("\n   æ³¨å†Œçš„æ£€æŸ¥ç‚¹:")
                for step_id, cp in list(checker.checkpoints.items())[:3]:
                    print(f"      - [{cp.artifact_type}] {cp.description[:40]}...")

            if checker.violations:
                print("\n   è¿è§„è®°å½•:")
                for v in checker.violations[-3:]:
                    print(f"      - [{v.severity}] {v.description[:50]}...")

        # 7. æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        print("\n" + "=" * 70)
        print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
        print("=" * 70)

        success_count = sum(1 for log in execution_log if log.get("success"))
        total_count = len(execution_log)
        print(f"\n   æ­¥éª¤å®Œæˆ: {success_count}/{total_count}")

        if "design_api" in final_results:
            endpoints = final_results["design_api"].get("endpoints", [])
            print(f"   API ç«¯ç‚¹: {len(endpoints)} ä¸ª")

        if "generate_model" in final_results:
            models = final_results["generate_model"].get("model_definitions", {})
            print(f"   æ•°æ®æ¨¡å‹: {len(models)} ä¸ª")

        if "implement_service" in final_results:
            impl = final_results["implement_service"].get("implemented_endpoints", [])
            print(f"   å®ç°ç«¯ç‚¹: {len(impl)} ä¸ª")

        if "review_code" in final_results:
            review = final_results["review_code"].get("review_result", {})
            print(f"   ä»£ç ä¸€è‡´æ€§: {review.get('consistency_score', 0):.0%}")
            print(f"   ä»£ç è´¨é‡: {review.get('quality_score', 0):.0%}")

        print("\n" + "=" * 70)

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()

    finally:
        await llm_client.close()


if __name__ == "__main__":
    asyncio.run(main())
