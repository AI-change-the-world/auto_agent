"""
å®Œæ•´åŠŸèƒ½æ¼”ç¤º

æµ‹è¯•å†…å®¹:
1. LLM çœŸå®è°ƒç”¨å’Œé—®ç­”æ•ˆæœ
2. è‡ªåŠ¨ä¸Šä¸‹æ–‡å‹ç¼©
3. ä¼šè¯ç®¡ç†å’Œå¤šè½®å¯¹è¯
4. æ„å›¾è·¯ç”±
5. åˆ†ç±»è®°å¿†ç³»ç»Ÿ
6. æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ
"""

import asyncio
import os
from typing import Any, Dict

from auto_agent import (
    BaseTool,
    CategorizedMemory,
    ExecutionPlan,
    ExecutionReportGenerator,
    IntentRouter,
    OpenAIClient,
    PlanStep,
    SessionManager,
    ShortTermMemory,
    SubTaskResult,
    ToolDefinition,
    ToolParameter,
)

# ==================== é…ç½® ====================


def get_llm_client():
    """è·å– LLM å®¢æˆ·ç«¯"""
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® OPENAI_API_KEY æˆ– DEEPSEEK_API_KEY")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•")
        return None

    base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")
    model = os.getenv("OPENAI_MODEL", "deepseek-chat")

    return OpenAIClient(
        api_key=api_key,
        base_url=base_url,
        model=model,
        timeout=120.0,
    )


# ==================== æµ‹è¯•å·¥å…· ====================


class AnalyzeInputTool(BaseTool):
    """åˆ†æç”¨æˆ·è¾“å…¥å·¥å…· - ä½¿ç”¨ LLM"""

    def __init__(self, llm_client):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="analyze_input",
            description="åˆ†æç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«æ„å›¾ã€ä¸»é¢˜å’Œå…³é”®ä¿¡æ¯",
            parameters=[
                ToolParameter(
                    name="query", type="string", description="ç”¨æˆ·è¾“å…¥", required=True
                ),
            ],
            category="analysis",
        )

    async def execute(self, query: str, **kwargs) -> Dict[str, Any]:
        if not self.llm_client:
            return {"success": False, "error": "LLM client not available"}

        prompt = f"""åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

ç”¨æˆ·è¾“å…¥: {query}

è¯·è¿”å› JSON æ ¼å¼:
{{
    "intent": "ç”¨æˆ·æ„å›¾ï¼ˆå¦‚ï¼šå†™ä½œã€æŸ¥è¯¢ã€åˆ†æç­‰ï¼‰",
    "topic": "ä¸»é¢˜",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "document_type": "æ–‡æ¡£ç±»å‹ï¼ˆå¦‚ï¼šæŠ¥å‘Šã€ç¬”è®°ã€æ€»ç»“ç­‰ï¼‰"
}}"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}], temperature=0.3
            )

            import json
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result["success"] = True
                return result

            return {
                "success": True,
                "intent": "å†™ä½œ",
                "topic": query[:50],
                "keywords": [],
                "raw_response": response,
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


class SearchTool(BaseTool):
    """æ¨¡æ‹Ÿæœç´¢å·¥å…· - è¿”å›å¤§é‡æ•°æ®ç”¨äºæµ‹è¯•å‹ç¼©"""

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="search_documents",
            description="æœç´¢ç›¸å…³æ–‡æ¡£",
            parameters=[
                ToolParameter(
                    name="query", type="string", description="æœç´¢æŸ¥è¯¢", required=True
                ),
                ToolParameter(
                    name="size", type="integer", description="è¿”å›æ•°é‡", required=False
                ),
            ],
            category="retrieval",
        )

    async def execute(self, query: str, size: int = 10, **kwargs) -> Dict[str, Any]:
        # æ¨¡æ‹Ÿè¿”å›å¤§é‡æ–‡æ¡£æ•°æ®ï¼ˆç”¨äºæµ‹è¯•å‹ç¼©ï¼‰
        documents = []
        for i in range(size):
            documents.append(
                {
                    "id": f"doc_{i}",
                    "title": f"æ–‡æ¡£{i}: å…³äº{query}çš„ç ”ç©¶",
                    "content": f"è¿™æ˜¯ä¸€ç¯‡å…³äº{query}çš„è¯¦ç»†æ–‡æ¡£å†…å®¹ã€‚" * 50,  # å¤§é‡å†…å®¹
                    "author": f"ä½œè€…{i}",
                    "date": "2024-01-01",
                    "score": 0.95 - i * 0.05,
                    "metadata": {
                        "category": "ç ”ç©¶",
                        "tags": ["AI", "æŠ€æœ¯", query],
                        "word_count": 5000 + i * 100,
                    },
                }
            )

        return {
            "success": True,
            "document_ids": [d["id"] for d in documents],
            "documents": documents,
            "total_count": len(documents),
            "query": query,
        }


class GenerateOutlineTool(BaseTool):
    """å¤§çº²ç”Ÿæˆå·¥å…· - ä½¿ç”¨ LLM"""

    def __init__(self, llm_client):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="generate_outline",
            description="æ ¹æ®ä¸»é¢˜ç”Ÿæˆæ–‡æ¡£å¤§çº²",
            parameters=[
                ToolParameter(
                    name="topic", type="string", description="æ–‡æ¡£ä¸»é¢˜", required=True
                ),
                ToolParameter(
                    name="doc_type",
                    type="string",
                    description="æ–‡æ¡£ç±»å‹",
                    required=False,
                ),
            ],
            category="document",
        )

    async def execute(
        self, topic: str, doc_type: str = "æŠ¥å‘Š", **kwargs
    ) -> Dict[str, Any]:
        if not self.llm_client:
            return {"success": False, "error": "LLM client not available"}

        prompt = f"""ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸€ä¸ª{doc_type}çš„å¤§çº²ã€‚

ä¸»é¢˜: {topic}

è¯·è¿”å› JSON æ ¼å¼çš„å¤§çº²:
{{
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "sections": [
        {{"title": "ç« èŠ‚æ ‡é¢˜", "subsections": ["å°èŠ‚1", "å°èŠ‚2"]}}
    ]
}}"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}], temperature=0.5
            )

            import json
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                outline = json.loads(json_match.group())
                return {"success": True, "outline": outline}

            return {
                "success": True,
                "outline": {
                    "title": f"å…³äº{topic}çš„{doc_type}",
                    "sections": [{"title": "æ¦‚è¿°", "subsections": []}],
                },
                "raw_response": response,
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


class ComposeDocumentTool(BaseTool):
    """æ–‡æ¡£æ’°å†™å·¥å…· - ä½¿ç”¨ LLM"""

    def __init__(self, llm_client):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="compose_document",
            description="æ ¹æ®å¤§çº²æ’°å†™æ–‡æ¡£",
            parameters=[
                ToolParameter(
                    name="outline", type="object", description="æ–‡æ¡£å¤§çº²", required=True
                ),
                ToolParameter(
                    name="context",
                    type="string",
                    description="å‚è€ƒä¸Šä¸‹æ–‡",
                    required=False,
                ),
            ],
            category="document",
        )

    async def execute(
        self, outline: Dict, context: str = "", **kwargs
    ) -> Dict[str, Any]:
        if not self.llm_client:
            return {"success": False, "error": "LLM client not available"}

        title = outline.get("title", "æœªå‘½åæ–‡æ¡£")
        sections = outline.get("sections", [])

        prompt = f"""æ ¹æ®ä»¥ä¸‹å¤§çº²æ’°å†™æ–‡æ¡£å†…å®¹ã€‚

æ ‡é¢˜: {title}
å¤§çº²: {sections}

å‚è€ƒä¿¡æ¯: {context[:500] if context else "æ— "}

è¯·ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜å’Œå„ç« èŠ‚ã€‚"""

        try:
            response = await self.llm_client.chat(
                [{"role": "user", "content": prompt}], temperature=0.7, max_tokens=2000
            )

            return {
                "success": True,
                "document": {
                    "title": title,
                    "content": response,
                    "word_count": len(response),
                },
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


# ==================== æµ‹è¯•å‡½æ•° ====================


async def test_llm_basic(llm_client):
    """æµ‹è¯• 1: LLM åŸºç¡€é—®ç­”"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• 1: LLM åŸºç¡€é—®ç­”")
    print("=" * 60)

    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿç”¨ä¸€å¥è¯å›ç­”ã€‚",
        "Python å’Œ Java çš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿç®€è¦å›ç­”ã€‚",
    ]

    for q in questions:
        print(f"\nâ“ é—®é¢˜: {q}")
        try:
            response = await llm_client.chat(
                [{"role": "user", "content": q}], temperature=0.7, max_tokens=200
            )
            print(f"âœ… å›ç­”: {response[:300]}...")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


async def test_context_compression(llm_client):
    """æµ‹è¯• 2: ä¸Šä¸‹æ–‡å‹ç¼©"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• 2: ä¸Šä¸‹æ–‡å‹ç¼©")
    print("=" * 60)

    # åˆå§‹åŒ–çŸ­æœŸè®°å¿†
    stm = ShortTermMemory(max_context_chars=5000)

    # æ¨¡æ‹Ÿæ‰§è¡Œå†å²ï¼ˆåŒ…å«å¤§é‡æ•°æ®ï¼‰
    step_history = []

    # æœç´¢å·¥å…·è¿”å›å¤§é‡æ–‡æ¡£
    search_tool = SearchTool()
    search_result = await search_tool.execute(query="äººå·¥æ™ºèƒ½", size=20)

    step_history.append(
        {
            "step": 1,
            "name": "search_documents",
            "description": "æœç´¢ç›¸å…³æ–‡æ¡£",
            "result": search_result,
        }
    )

    # æ¨¡æ‹Ÿåˆ†æç»“æœ
    step_history.append(
        {
            "step": 2,
            "name": "analyze_input",
            "description": "åˆ†æç”¨æˆ·è¾“å…¥",
            "result": {
                "success": True,
                "intent": "å†™ä½œ",
                "topic": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
                "keywords": ["AI", "åŒ»ç–—", "è¯Šæ–­", "æ²»ç–—"],
            },
        }
    )

    # åŸå§‹æ•°æ®å¤§å°
    import json

    original_size = len(json.dumps(step_history, ensure_ascii=False))
    print(f"\nğŸ“Š åŸå§‹æ•°æ®å¤§å°: {original_size} å­—ç¬¦")

    # å‹ç¼©çŠ¶æ€
    state = {
        "inputs": {"query": "å†™ä¸€ç¯‡AIåŒ»ç–—æŠ¥å‘Š"},
        "documents": search_result["documents"],
        "document_ids": search_result["document_ids"],
    }

    compressed = stm.summarize_state(
        state=state,
        step_history=step_history,
        target_tool_name="compose_document",
        max_steps=5,
    )

    compressed_size = len(compressed)
    compression_ratio = (1 - compressed_size / original_size) * 100

    print(f"ğŸ“Š å‹ç¼©åå¤§å°: {compressed_size} å­—ç¬¦")
    print(f"ğŸ“Š å‹ç¼©ç‡: {compression_ratio:.1f}%")
    print("\nğŸ“„ å‹ç¼©åå†…å®¹é¢„è§ˆ:")
    print("-" * 40)
    print(compressed[:1000])

    # éªŒè¯å‹ç¼©åä»å¯ç”¨äº LLM
    if llm_client:
        print("\nğŸ”„ ä½¿ç”¨å‹ç¼©ä¸Šä¸‹æ–‡è°ƒç”¨ LLM...")
        prompt = f"""åŸºäºä»¥ä¸‹æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œæ€»ç»“å·²å®Œæˆçš„å·¥ä½œï¼š

{compressed[:2000]}

è¯·ç®€è¦æ€»ç»“ã€‚"""

        try:
            response = await llm_client.chat(
                [{"role": "user", "content": prompt}], max_tokens=300
            )
            print(f"âœ… LLM å“åº”: {response[:500]}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


async def test_tool_chain_with_llm(llm_client):
    """æµ‹è¯• 3: å·¥å…·é“¾æ‰§è¡Œï¼ˆçœŸå® LLM è°ƒç”¨ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• 3: å·¥å…·é“¾æ‰§è¡Œï¼ˆçœŸå® LLM è°ƒç”¨ï¼‰")
    print("=" * 60)

    # åˆå§‹åŒ–å·¥å…·
    analyze_tool = AnalyzeInputTool(llm_client)
    search_tool = SearchTool()
    outline_tool = GenerateOutlineTool(llm_client)
    compose_tool = ComposeDocumentTool(llm_client)

    query = "å¸®æˆ‘å†™ä¸€ç¯‡å…³äºå¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸåº”ç”¨çš„è°ƒç ”æŠ¥å‘Š"
    print(f"\nğŸ“‹ ç”¨æˆ·æŸ¥è¯¢: {query}")

    results = []
    state = {}

    # Step 1: åˆ†æè¾“å…¥
    print("\nğŸ”§ Step 1: åˆ†æç”¨æˆ·è¾“å…¥...")
    result1 = await analyze_tool.execute(query=query)
    results.append(
        SubTaskResult(
            step_id="1", success=result1.get("success", False), output=result1
        )
    )
    if result1.get("success"):
        state["topic"] = result1.get("topic", query)
        state["intent"] = result1.get("intent", "å†™ä½œ")
        state["doc_type"] = result1.get("document_type", "æŠ¥å‘Š")
        print(f"   âœ… æ„å›¾: {state['intent']}, ä¸»é¢˜: {state['topic']}")
    else:
        print(f"   âŒ å¤±è´¥: {result1.get('error')}")

    # Step 2: æœç´¢æ–‡æ¡£
    print("\nğŸ”§ Step 2: æœç´¢ç›¸å…³æ–‡æ¡£...")
    result2 = await search_tool.execute(query=state.get("topic", query), size=5)
    results.append(
        SubTaskResult(
            step_id="2", success=result2.get("success", False), output=result2
        )
    )
    if result2.get("success"):
        state["documents"] = result2.get("documents", [])
        state["document_ids"] = result2.get("document_ids", [])
        print(f"   âœ… æ‰¾åˆ° {result2.get('total_count', 0)} ç¯‡æ–‡æ¡£")

    # Step 3: ç”Ÿæˆå¤§çº²
    print("\nğŸ”§ Step 3: ç”Ÿæˆæ–‡æ¡£å¤§çº²...")
    result3 = await outline_tool.execute(
        topic=state.get("topic", query),
        doc_type=state.get("doc_type", "æŠ¥å‘Š"),
    )
    results.append(
        SubTaskResult(
            step_id="3", success=result3.get("success", False), output=result3
        )
    )
    if result3.get("success"):
        state["outline"] = result3.get("outline", {})
        outline = state["outline"]
        print(f"   âœ… å¤§çº²æ ‡é¢˜: {outline.get('title', 'N/A')}")
        sections = outline.get("sections", [])
        for s in sections[:3]:
            print(f"      - {s.get('title', 'N/A')}")
        if len(sections) > 3:
            print(f"      ... å…± {len(sections)} ä¸ªç« èŠ‚")
    else:
        print(f"   âŒ å¤±è´¥: {result3.get('error')}")

    # Step 4: æ’°å†™æ–‡æ¡£
    print("\nğŸ”§ Step 4: æ’°å†™æ–‡æ¡£...")
    if state.get("outline"):
        # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å‹ç¼©ï¼‰
        stm = ShortTermMemory()
        context = stm.summarize_state(
            state=state,
            step_history=[],
            max_steps=3,
        )

        result4 = await compose_tool.execute(
            outline=state["outline"],
            context=context,
        )
        results.append(
            SubTaskResult(
                step_id="4", success=result4.get("success", False), output=result4
            )
        )
        if result4.get("success"):
            doc = result4.get("document", {})
            state["document"] = doc
            print(f"   âœ… æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå­—æ•°: {doc.get('word_count', 0)}")
            print("\nğŸ“„ æ–‡æ¡£é¢„è§ˆ:")
            print("-" * 40)
            content = doc.get("content", "")
            print(content[:1500] if content else "æ— å†…å®¹")
            if len(content) > 1500:
                print(f"\n... (å…± {len(content)} å­—ç¬¦)")
        else:
            print(f"   âŒ å¤±è´¥: {result4.get('error')}")

    # ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š")
    print("=" * 60)

    plan = ExecutionPlan(
        intent=state.get("intent", "å†™ä½œ"),
        subtasks=[
            PlanStep(id="1", tool="analyze_input", description="åˆ†æç”¨æˆ·è¾“å…¥"),
            PlanStep(id="2", tool="search_documents", description="æœç´¢ç›¸å…³æ–‡æ¡£"),
            PlanStep(id="3", tool="generate_outline", description="ç”Ÿæˆæ–‡æ¡£å¤§çº²"),
            PlanStep(id="4", tool="compose_document", description="æ’°å†™æ–‡æ¡£"),
        ],
    )

    report_data = ExecutionReportGenerator.generate_report_data(
        agent_name="æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“",
        query=query,
        plan=plan,
        results=results,
        state=state,
    )

    stats = report_data["statistics"]
    print("\nğŸ“ˆ ç»Ÿè®¡:")
    print(f"   æ€»æ­¥éª¤: {stats['total_steps']}")
    print(f"   æˆåŠŸ: {stats['successful_steps']}")
    print(f"   å¤±è´¥: {stats['failed_steps']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']}%")

    print("\nğŸ“Š Mermaid æµç¨‹å›¾:")
    print(report_data["mermaid_diagram"])

    return results, state


async def test_session_and_memory(llm_client):
    """æµ‹è¯• 4: ä¼šè¯ç®¡ç†å’Œè®°å¿†ç³»ç»Ÿ"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• 4: ä¼šè¯ç®¡ç†å’Œè®°å¿†ç³»ç»Ÿ")
    print("=" * 60)

    # åˆå§‹åŒ–
    session_manager = SessionManager(default_ttl=300)
    memory = CategorizedMemory(storage_path=None)

    user_id = "test_user_001"

    # åˆ›å»ºä¼šè¯
    print("\nğŸ“ åˆ›å»ºä¼šè¯...")
    session = session_manager.create_session(
        user_id=user_id,
        initial_query="å¸®æˆ‘å†™ä¸€ç¯‡æŠ€æœ¯æ–‡æ¡£",
    )
    print(f"   ä¼šè¯ID: {session.session_id}")
    print(f"   çŠ¶æ€: {session.status.value}")

    # è®°å½•ç”¨æˆ·åå¥½
    print("\nğŸ’¾ è®°å½•ç”¨æˆ·åå¥½...")
    memory.set_preference(user_id, "language", "ä¸­æ–‡")
    memory.set_preference(user_id, "style", "ä¸“ä¸š")
    memory.set_preference(user_id, "doc_format", "markdown")

    # è®°å½•ç”¨æˆ·è¡Œä¸º
    memory.add_behavior(user_id, "start_task", {"query": "å†™æŠ€æœ¯æ–‡æ¡£"})

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    print("\nğŸ’¬ æ¨¡æ‹Ÿå¤šè½®å¯¹è¯...")

    # ç¬¬ä¸€è½®
    session_manager.add_message(
        session.session_id, "assistant", "å¥½çš„ï¼Œè¯·é—®æ‚¨æƒ³å†™ä»€ä¹ˆä¸»é¢˜çš„æŠ€æœ¯æ–‡æ¡£ï¼Ÿ"
    )

    # ç­‰å¾…ç”¨æˆ·è¾“å…¥
    session_manager.wait_for_input(session.session_id, "è¯·æä¾›æ–‡æ¡£ä¸»é¢˜")
    print(f"   çŠ¶æ€: {session_manager.get_session(session.session_id).status.value}")

    # ç”¨æˆ·å›å¤
    session_manager.resume_session(session.session_id, "å…³äº Python å¼‚æ­¥ç¼–ç¨‹çš„æ•™ç¨‹")
    print(f"   çŠ¶æ€: {session_manager.get_session(session.session_id).status.value}")

    # ç»§ç»­å¯¹è¯
    session_manager.add_message(
        session.session_id, "assistant", "å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ç”Ÿæˆ Python å¼‚æ­¥ç¼–ç¨‹æ•™ç¨‹..."
    )

    # è®°å½•åé¦ˆ
    memory.add_feedback(user_id, "å“åº”é€Ÿåº¦å¾ˆå¿«", rating=5)

    # æ·»åŠ çŸ¥è¯†
    memory.add_knowledge(user_id, "ç”¨æˆ·ç†Ÿæ‚‰ Python ç¼–ç¨‹", tags=["æŠ€èƒ½", "Python"])

    # è·å–å¯¹è¯å†å²
    print("\nğŸ“œ å¯¹è¯å†å²:")
    history = session_manager.get_conversation_history(session.session_id)
    for msg in history:
        print(f"   [{msg['role']}]: {msg['content'][:50]}...")

    # è·å–ç”¨æˆ·ä¸Šä¸‹æ–‡
    print("\nğŸ§  ç”¨æˆ·ä¸Šä¸‹æ–‡æ‘˜è¦:")
    context = memory.get_context_summary(user_id)
    print(context)

    # æœç´¢è®°å¿†
    print("\nğŸ” æœç´¢è®°å¿† 'Python':")
    results = memory.search(user_id, "Python")
    for item in results:
        print(f"   - [{item.category.value}] {item.key}: {item.value}")

    # å®Œæˆä¼šè¯
    session_manager.complete_session(session.session_id, "æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼")
    final_session = session_manager.get_session(session.session_id)
    print(f"\nâœ… ä¼šè¯å®Œæˆï¼ŒçŠ¶æ€: {final_session.status.value}")
    print(f"   æ¶ˆæ¯æ•°: {len(final_session.messages)}")


async def test_intent_routing(llm_client):
    """æµ‹è¯• 5: æ„å›¾è·¯ç”±"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• 5: æ„å›¾è·¯ç”±")
    print("=" * 60)

    # åˆå§‹åŒ–è·¯ç”±å™¨
    router = IntentRouter(llm_client=llm_client, default_handler="chat")

    # æ³¨å†Œå¤„ç†å™¨
    router.register(
        name="writer",
        description="æ–‡æ¡£å†™ä½œï¼ŒåŒ…æ‹¬æŠ¥å‘Šã€æ–‡ç« ã€ç¬”è®°ç­‰",
        keywords=["å†™", "æ’°å†™", "æ–‡æ¡£", "æŠ¥å‘Š", "æ–‡ç« ", "ç¬”è®°", "æ€»ç»“"],
    )
    router.register(
        name="search",
        description="ä¿¡æ¯æ£€ç´¢å’Œæœç´¢",
        keywords=["æœç´¢", "æŸ¥æ‰¾", "æ£€ç´¢", "æŸ¥è¯¢", "æ‰¾"],
    )
    router.register(
        name="analysis",
        description="æ•°æ®åˆ†æå’Œç»Ÿè®¡",
        keywords=["åˆ†æ", "ç»Ÿè®¡", "æ•°æ®", "è¶‹åŠ¿", "å¯¹æ¯”"],
    )
    router.register(
        name="qa",
        description="é—®ç­”å’ŒçŸ¥è¯†æŸ¥è¯¢",
        keywords=["ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "è§£é‡Š"],
    )
    router.register(
        name="chat",
        description="æ—¥å¸¸å¯¹è¯å’Œé—²èŠ",
        keywords=[],
    )

    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        "å¸®æˆ‘å†™ä¸€ç¯‡å…³äºAIçš„è°ƒç ”æŠ¥å‘Š",
        "æœç´¢æœ€æ–°çš„æœºå™¨å­¦ä¹ è®ºæ–‡",
        "åˆ†æè¿™äº›é”€å”®æ•°æ®çš„è¶‹åŠ¿",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "å¸®æˆ‘æ€»ç»“ä¸€ä¸‹è¿™ç¯‡æ–‡ç« çš„è¦ç‚¹",
    ]

    print("\nğŸ”€ è·¯ç”±æµ‹è¯•:")
    for query in test_queries:
        result = await router.route(query)
        print(f"\n   ğŸ“‹ æŸ¥è¯¢: {query}")
        print(f"   ğŸ¯ è·¯ç”±: {result.handler_name} (ç½®ä¿¡åº¦: {result.confidence:.2f})")
        print(f"   ğŸ’¡ æ„å›¾: {result.intent}")
        if result.reasoning:
            print(f"   ğŸ“ ç†ç”±: {result.reasoning}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ Auto-Agent å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)

    # è·å– LLM å®¢æˆ·ç«¯
    llm_client = get_llm_client()
    has_llm = llm_client is not None

    if has_llm:
        print("\nâœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("\nâš ï¸  æ— æ³•è·å– LLM å®¢æˆ·ç«¯ï¼Œå°†è¿è¡Œä¸éœ€è¦ LLM çš„æµ‹è¯•")

    try:
        if has_llm:
            # æµ‹è¯• 1: LLM åŸºç¡€é—®ç­”
            await test_llm_basic(llm_client)

        # æµ‹è¯• 2: ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆä¸éœ€è¦ LLM ä¹Ÿå¯ä»¥æµ‹è¯•å‹ç¼©é€»è¾‘ï¼‰
        await test_context_compression(llm_client)

        if has_llm:
            # æµ‹è¯• 3: å·¥å…·é“¾æ‰§è¡Œ
            await test_tool_chain_with_llm(llm_client)

        # æµ‹è¯• 4: ä¼šè¯ç®¡ç†å’Œè®°å¿†ï¼ˆä¸éœ€è¦ LLMï¼‰
        await test_session_and_memory(llm_client)

        # æµ‹è¯• 5: æ„å›¾è·¯ç”±ï¼ˆå…³é”®è¯åŒ¹é…ä¸éœ€è¦ LLMï¼‰
        await test_intent_routing(llm_client)

        print("\n" + "=" * 60)
        if has_llm:
            print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        else:
            print("âœ… é LLM æµ‹è¯•å®Œæˆ! è®¾ç½® API Key åå¯è¿è¡Œå®Œæ•´æµ‹è¯•")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()

    finally:
        if has_llm:
            await llm_client.close()


if __name__ == "__main__":
    asyncio.run(main())
