"""
å¼€æ”¾æ€§é«˜éš¾åº¦ Demoï¼šå¯ç»‘å®šå‚æ•° + LLM æ¨æ–­å‚æ•°(llm_infer) + é‡è¯• + åæ€æ”¹è¿› + replan

ç‰¹ç‚¹ï¼š
- å·¥å…·æ•°é‡å°‘ï¼ˆ3 ä¸ªå·¥å…·ï¼šgenerate_memo / verify_memo / reflect_feedbackï¼‰
- è®¡åˆ’æ­¥éª¤å°‘ï¼ˆ5 æ­¥å›ºå®šè®¡åˆ’ï¼Œverify å¤±è´¥ä¼šå›åˆ° reflect->generate å¾ªç¯ï¼‰
- å‚æ•°ç»‘å®šï¼š
  - requirements æ¥è‡ª user_inputï¼ˆé™æ€ç»‘å®šï¼‰
  - memo æ¥è‡ª step_outputï¼ˆé™æ€ç»‘å®šï¼‰
  - rubric / feedback ç­‰ç”±ç³»ç»Ÿåœ¨è¿è¡Œæ—¶ llm_inferï¼ˆBindingPlanner ä¼šç»™å‡º generated / fallback=llm_inferï¼‰
- é‡è¯•ï¼šgenerate_memo äººä¸ºæ¨¡æ‹Ÿä¸€æ¬¡ transient failureï¼Œä¾é  on_fail_strategy="é‡è¯•" è§¦å‘é‡è¯•
- åæ€ï¼šreflect_feedback ä¼šåŸºäº verify çš„è¾“å‡ºï¼ˆå³ä½¿ expectation fail ä¹Ÿä¼šè¢«å†™å…¥ stateï¼‰ç”Ÿæˆé’ˆå¯¹æ€§æ”¹è¿›æŒ‡ä»¤
- replanï¼šæ‰§è¡Œå¼•æ“åœ¨å¤±è´¥æ¨¡å¼ä¸‹ä¼šè§¦å‘ replanï¼ˆå¯åœ¨äº‹ä»¶ä¸­è§‚å¯Ÿ stage_replanï¼‰

è¿è¡Œï¼š
  python examples/adaptive_memo_demo.py

ç¯å¢ƒå˜é‡ï¼š
  OPENAI_API_KEY / DEEPSEEK_API_KEY
  OPENAI_BASE_URL (å¯é€‰ï¼Œé»˜è®¤ deepseek)
  OPENAI_MODEL (å¯é€‰ï¼Œé»˜è®¤ deepseek-chat)
"""

import asyncio
import json
import os
import re
from typing import Any, Dict, Optional

from auto_agent import AutoAgent, BaseTool, OpenAIClient, ToolDefinition, ToolParameter, ToolRegistry


def get_llm_client() -> Optional[OpenAIClient]:
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        return None
    base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")
    model = os.getenv("OPENAI_MODEL", "deepseek-chat")
    return OpenAIClient(api_key=api_key, base_url=base_url, model=model, timeout=120.0)


def _validate_verify_output(result, expectations, state, mode, llm_client, db):
    """verify_memo çš„ validate_functionï¼špassed==True æ‰ç®—æ»¡è¶³æœŸæœ›ã€‚"""
    passed = bool(result.get("passed"))
    if passed:
        return True, "é€šè¿‡éªŒè¯"
    issues = result.get("issues") or []
    if isinstance(issues, list) and issues:
        return False, f"æœªé€šè¿‡éªŒè¯: {issues[0]}"
    return False, "æœªé€šè¿‡éªŒè¯"


class GenerateMemoTool(BaseTool):
    """ç”Ÿæˆå¤‡å¿˜å½•ï¼ˆrubric/feedback å¯ç”±ç³»ç»Ÿæ¨æ–­ï¼‰"""

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client
        self._fail_once = True

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="generate_memo",
            description=(
                "æ ¹æ® requirements + rubric ç”Ÿæˆä¸€ä»½ä¸­æ–‡å†³ç­–å¤‡å¿˜å½•ã€‚"
                "rubric æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å« required_sections/min_risks/max_words/tone ç­‰ã€‚"
                "å¦‚æœç”¨æˆ·æœªæä¾› rubric æˆ– feedbackï¼Œåº”ç”±ç³»ç»Ÿåœ¨è¿è¡Œæ—¶æ¨æ–­å¹¶è¡¥å…¨ã€‚"
            ),
            parameters=[
                ToolParameter(
                    name="requirements",
                    type="string",
                    description="ç”¨æˆ·çš„ä»»åŠ¡/èƒŒæ™¯/ç›®æ ‡ï¼ˆæ¥è‡ªè¾“å…¥ï¼‰",
                    required=True,
                ),
                ToolParameter(
                    name="rubric",
                    type="object",
                    description=(
                        "è¯„ä¼°ä¸å†™ä½œè§„åˆ™(JSON)ã€‚ç¤ºä¾‹: "
                        '{"required_sections":["èƒŒæ™¯","å†³ç­–","æ–¹æ¡ˆå¯¹æ¯”","é£é™©æ¸…å•","ä¸‹ä¸€æ­¥"],'
                        '"min_risks":5,"max_words":650,"tone":"ä¸“ä¸š"}ã€‚'
                        "è‹¥ç”¨æˆ·æœªæä¾›ï¼Œå¿…é¡»ç”±ç³»ç»Ÿç”Ÿæˆã€‚"
                    ),
                    required=True,
                ),
                ToolParameter(
                    name="feedback",
                    type="string",
                    description="æ¥è‡ªä¸Šä¸€æ¬¡éªŒè¯/åæ€çš„æ”¹è¿›æŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰",
                    required=False,
                ),
                ToolParameter(
                    name="previous_memo",
                    type="string",
                    description="ä¸Šä¸€ç‰ˆå¤‡å¿˜å½•ï¼ˆå¯é€‰ï¼Œç”¨äºå¢é‡æ”¹å†™ï¼‰",
                    required=False,
                ),
            ],
            output_schema={
                "memo": {"type": "string"},
                "rubric": {"type": "object"},
                "used_feedback": {"type": "string"},
            },
            # ç»™æ‰§è¡Œå™¨å…œåº•ï¼šå³ä¾¿æ²¡æœ‰ binding_planï¼Œä¹Ÿèƒ½ä» state ä¸­å–åˆ°è¿™äº›å€¼
            param_aliases={
                "requirements": "inputs.requirements",
                "rubric": "rubric",
                "feedback": "feedback",
                "previous_memo": "memo",
            },
        )

    async def execute(
        self,
        requirements: str,
        rubric: Dict[str, Any],
        feedback: str = "",
        previous_memo: str = "",
        **kwargs,
    ) -> Dict[str, Any]:
        # æ¨¡æ‹Ÿä¸€æ¬¡ç¬æ€å¤±è´¥ï¼šè®© demo èƒ½çœ‹åˆ° retry
        if self._fail_once:
            self._fail_once = False
            return {"success": False, "error": "transient_failure: simulate retry once"}

        rubric_json = json.dumps(rubric, ensure_ascii=False, indent=2)
        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±æŠ€æœ¯è´Ÿè´£äººï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚æ’°å†™ä¸€ä»½â€œå†³ç­–å¤‡å¿˜å½•â€(ä¸­æ–‡)ã€‚

ã€requirementsã€‘
{requirements}

ã€rubricï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ã€‘
{rubric_json}

ã€ä¸Šä¸€æ¬¡åé¦ˆï¼ˆå¦‚æœæœ‰åˆ™å¿…é¡»é€æ¡è½å®ï¼‰ã€‘
{feedback if feedback else "æ— "}

ã€ä¸Šä¸€ç‰ˆå¤‡å¿˜å½•ï¼ˆå¯é€‰å‚è€ƒï¼Œé¿å…é‡å¤ç©ºè¯ï¼‰ã€‘
{previous_memo[:1500] if previous_memo else "æ— "}

å†™ä½œè¦æ±‚ï¼š
1) ä½¿ç”¨ rubric.required_sections çš„æ ‡é¢˜ä½œä¸ºä¸€çº§æ ‡é¢˜ï¼ˆæŒ‰è¯¥é¡ºåºï¼‰
2) â€œé£é™©æ¸…å•â€è‡³å°‘åˆ—å‡º rubric.min_risks æ¡é£é™©ï¼Œæ¯æ¡åŒ…å«: é£é™©/ä¸¥é‡åº¦/ç¼“è§£æªæ–½
3) æ§åˆ¶å…¨æ–‡ä¸è¶…è¿‡ rubric.max_words ä¸ªä¸­æ–‡å­—ç¬¦ï¼ˆå°½é‡ç²¾ç‚¼ï¼‰
4) è¯­æ°”/é£æ ¼ç¬¦åˆ rubric.tone
5) ä¸è¦è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºè§£é‡Šï¼Œç›´æ¥è¾“å‡ºå¤‡å¿˜å½•æ­£æ–‡
"""

        memo = await self.llm_client.chat(
            [{"role": "user", "content": prompt}],
            temperature=0.4,
            trace_purpose="tool_generate_memo",
        )

        return {
            "success": True,
            "memo": memo.strip(),
            "rubric": rubric,
            "used_feedback": feedback or "",
        }


class VerifyMemoTool(BaseTool):
    """LLM åˆ¤å®šéªŒè¯ï¼ˆæŠŠéªŒè¯æŠ¥å‘Šå†™å…¥ stateï¼Œä¾¿äºåç»­åæ€/æ”¹å†™ï¼‰"""

    def __init__(self, llm_client: Optional[OpenAIClient] = None):
        # å¯é€‰ï¼šç”¨äºâ€œè¯­ä¹‰å‹å…œåº•åˆ¤å®šâ€ï¼ˆä»…åœ¨è§„åˆ™è§£æä¸å¯é æ—¶è§¦å‘ï¼‰
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="verify_memo",
            description="éªŒè¯å¤‡å¿˜å½•æ˜¯å¦æ»¡è¶³ rubricã€‚è¿”å› passed/issues/scoreï¼Œå¹¶é™„å¸¦ verification æŠ¥å‘Šã€‚",
            parameters=[
                ToolParameter(
                    name="memo",
                    type="string",
                    description="å¾…éªŒè¯çš„å¤‡å¿˜å½•",
                    required=True,
                ),
                ToolParameter(
                    name="rubric",
                    type="object",
                    description="è¯„ä¼°è§„åˆ™(JSON)ï¼Œæ¥è‡ª generate_memo è¾“å‡º",
                    required=True,
                ),
            ],
            output_schema={
                "passed": {"type": "boolean"},
                "issues": {"type": "array"},
                "score": {"type": "number"},
                "verification": {"type": "object"},
            },
            validate_function=_validate_verify_output,
            param_aliases={"memo": "memo", "rubric": "rubric"},
        )

    async def execute(self, memo: str, rubric: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        if not self.llm_client:
            return {
                "success": True,
                "passed": False,
                "issues": ["llm_client_missing_for_verify"],
                "score": 10.0,
                "verification": {"passed": False, "issues": ["llm_client_missing_for_verify"]},
            }

        min_risks = int(rubric.get("min_risks") or 0)
        max_words = int(rubric.get("max_words") or 0)

        rubric_json = json.dumps(rubric, ensure_ascii=False, indent=2)
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„â€œå¤‡å¿˜å½•è¯„å®¡å®˜â€ã€‚è¯·ä»…åŸºäº rubric åˆ¤æ–­ memo æ˜¯å¦é€šè¿‡ï¼Œå¹¶ç»™å‡ºå¯æ‰§è¡Œ issuesã€‚

è¦æ±‚ï¼š
1) ä½ å¿…é¡»è¾“å‡º JSONï¼ˆä¸è¦è¾“å‡ºå…¶å®ƒæ–‡å­—ï¼‰ï¼Œç»“æ„å¦‚ä¸‹ï¼š
{{
  "passed": true/false,
  "score": 0-100,
  "issues": ["..."],
  "risk_items": ["..."],  // ä½ è®¤ä¸ºå±äºã€Šé£é™©æ¸…å•ã€‹çš„ç‹¬ç«‹é£é™©æ¡ç›®
  "memo_length": <int>
}}
2) passed ä¸º true çš„å……è¦æ¡ä»¶ï¼šrubric.required_sections å…¨éƒ¨å‡ºç°ï¼›risk_items æ•°é‡ >= rubric.min_risksï¼›memo_length <= rubric.max_words
3) issues å¿…é¡»å…·ä½“ã€å¯æ“ä½œï¼ˆä¾‹å¦‚â€œé£é™©æ¡ç›®ä¸è¶³ï¼šç›®å‰3æ¡ï¼Œéœ€è¦>=5æ¡â€ï¼‰

ã€rubricã€‘
{rubric_json}

ã€memoã€‘
{memo}
"""
        resp = await self.llm_client.chat(
            [{"role": "user", "content": prompt}],
            temperature=0.0,
            trace_purpose="tool_verify_memo_llm_judge",
        )

        passed = False
        score = 10.0
        issues = ["judge_parse_failed"]
        risk_items: list[str] = []
        try:
            m = re.search(r"\{[\s\S]*\}", resp)
            if m:
                obj = json.loads(m.group(0))
                passed = bool(obj.get("passed"))
                score = float(obj.get("score", 0.0) or 0.0)
                issues_raw = obj.get("issues") or []
                if isinstance(issues_raw, list):
                    issues = [str(x) for x in issues_raw if str(x).strip()]
                risk_raw = obj.get("risk_items") or []
                if isinstance(risk_raw, list):
                    risk_items = [str(x) for x in risk_raw if str(x).strip()]
        except Exception:
            # ä¿ç•™é»˜è®¤å€¼
            pass

        # åŒä¿é™©ï¼šå¦‚æœ LLM æ²¡å¡« memo_lengthï¼Œå°±ç”¨æœ¬åœ°é•¿åº¦
        memo_len = len(memo or "")
        # ç”¨ rubric çº¦æŸå†å…œä¸€å±‚ï¼ˆé˜²æ­¢æ¨¡å‹è¾“å‡ºä¸ä¸€è‡´ï¼‰
        if memo_len > 0:
            if max_words > 0 and memo_len > max_words and all("å†…å®¹è¿‡é•¿" not in str(x) for x in issues):
                issues.append(f"å†…å®¹è¿‡é•¿: {memo_len} > {max_words}")
            if min_risks > 0 and len(risk_items) < min_risks and all("é£é™©æ¡æ•°ä¸è¶³" not in str(x) for x in issues):
                issues.append(f"é£é™©æ¡æ•°ä¸è¶³: {len(risk_items)} < {min_risks}")
            passed = (len(issues) == 0)

        verification = {
            "passed": passed,
            "issues": issues,
            "memo_length": memo_len,
            "min_risks": min_risks,
            "max_words": max_words,
            "risk_items_count": len(risk_items),
            "risk_count_method": "llm_only",
            "risk_items_preview": risk_items[:10],
        }

        # æ³¨æ„ï¼šè¿™é‡Œ success æ°¸è¿œ Trueï¼Œè®©è¾“å‡ºèƒ½å†™å…¥ stateï¼›
        # â€œæ˜¯å¦é€šè¿‡â€äº¤ç»™ validate_function + expectations æ¥å†³å®š step æˆè´¥ã€‚
        return {
            "success": True,
            "passed": passed,
            "issues": issues,
            "score": score,
            "verification": verification,
        }


class ReflectFeedbackTool(BaseTool):
    """åæ€ä¸Šä¸€æ­¥éªŒè¯ç»“æœï¼Œäº§å‡ºæ›´å…·ä½“çš„æ”¹è¿›æŒ‡ä»¤ï¼ˆç”¨äºä¸‹ä¸€è½® generate_memoï¼‰"""

    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client

    @property
    def definition(self) -> ToolDefinition:
        return ToolDefinition(
            name="reflect_feedback",
            description="æ ¹æ® verification æŠ¥å‘Šï¼Œåæ€å¤‡å¿˜å½•çš„ä¸è¶³å¹¶ç»™å‡ºé’ˆå¯¹æ€§æ”¹è¿›æŒ‡ä»¤ï¼ˆç”¨äºä¸‹ä¸€è½®ç”Ÿæˆï¼‰ã€‚",
            parameters=[
                ToolParameter(
                    name="requirements",
                    type="string",
                    description="åŸå§‹ requirements",
                    required=True,
                ),
                ToolParameter(
                    name="memo",
                    type="string",
                    description="ä¸Šä¸€ç‰ˆå¤‡å¿˜å½•",
                    required=True,
                ),
                ToolParameter(
                    name="verification",
                    type="object",
                    description="verify_memo çš„ verification æŠ¥å‘Šï¼ˆå³ä½¿éªŒè¯å¤±è´¥ä¹Ÿä¼šå†™å…¥ stateï¼‰",
                    required=True,
                ),
                ToolParameter(
                    name="rubric",
                    type="object",
                    description="rubricï¼ˆç”¨äºä¿æŒä¸€è‡´ï¼‰",
                    required=True,
                ),
            ],
            output_schema={"feedback": {"type": "string"}, "rubric": {"type": "object"}},
            param_aliases={
                "requirements": "inputs.requirements",
                "memo": "memo",
                "verification": "verification",
                "rubric": "rubric",
            },
        )

    async def execute(
        self,
        requirements: str,
        memo: str,
        verification: Dict[str, Any],
        rubric: Dict[str, Any],
        **kwargs,
    ) -> Dict[str, Any]:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªâ€œå¤‡å¿˜å½•è´¨é‡æ”¹è¿›åŠ©æ‰‹â€ã€‚è¯·æ ¹æ®éªŒè¯æŠ¥å‘Šï¼Œè¾“å‡ºä¸‹ä¸€è½®ç”Ÿæˆ memo çš„â€œå¯æ‰§è¡Œæ”¹è¿›æŒ‡ä»¤â€ã€‚

ã€requirementsã€‘
{requirements}

ã€rubricã€‘
{json.dumps(rubric, ensure_ascii=False, indent=2)}

ã€verification æŠ¥å‘Šã€‘
{json.dumps(verification, ensure_ascii=False, indent=2)}

ã€ä¸Šä¸€ç‰ˆ memoï¼ˆæˆªæ–­ï¼‰ã€‘
{memo[:1800]}

è¾“å‡ºè¦æ±‚ï¼š
1) åªè¾“å‡ºæ”¹è¿›æŒ‡ä»¤æ–‡æœ¬ï¼ˆä¸è¦ JSONï¼‰
2) å¿…é¡»é€æ¡å¯¹åº” verification.issuesï¼Œç»™å‡ºæ˜ç¡®çš„â€œåº”è¯¥è¡¥ä»€ä¹ˆ/åˆ ä»€ä¹ˆ/æ”¹ä»€ä¹ˆâ€
3) å¦‚æœå­—æ•°è¶…é™ï¼Œç»™å‡ºç²¾ç®€ç­–ç•¥ï¼ˆåˆ å†—ä½™ã€åˆå¹¶æ®µè½ã€æ”¹ä¸ºåˆ—è¡¨ï¼‰
4) ç»™å‡ºä¸€ä¸ªâ€œä¼˜å…ˆçº§é¡ºåºâ€ï¼ˆå…ˆä¿®æœ€å½±å“é€šè¿‡éªŒè¯çš„ç‚¹ï¼‰
"""
        feedback = await self.llm_client.chat(
            [{"role": "user", "content": prompt}],
            temperature=0.2,
            trace_purpose="tool_reflect_feedback",
        )

        return {"success": True, "feedback": feedback.strip(), "rubric": rubric}


async def main():
    llm_client = get_llm_client()
    if not llm_client:
        print("è¯·å…ˆè®¾ç½® OPENAI_API_KEY æˆ– DEEPSEEK_API_KEY")
        return

    registry = ToolRegistry()
    registry.register(GenerateMemoTool(llm_client))
    registry.register(VerifyMemoTool(llm_client))
    registry.register(ReflectFeedbackTool(llm_client))

    agent = AutoAgent(
        llm_client=llm_client,
        tool_registry=registry,
        agent_name="Adaptive Memo Agent",
        agent_description="å°‘å·¥å…· + å¤šè½®åæ€æ”¹è¿› + å¯é‡è¯•/å¯é‡è§„åˆ’ çš„å¼€æ”¾æ€§ demo",
    )

    # å¼€æ”¾æ€§ä»»åŠ¡ï¼šä½ ä¹Ÿå¯ä»¥æ›¿æ¢æˆè‡ªå·±æƒ³æµ‹çš„é«˜éš¾åº¦ä»»åŠ¡
    user_query = """
è¯·ä¸ºå›¢é˜Ÿå†™ä¸€ä»½â€œæŠ€æœ¯å†³ç­–å¤‡å¿˜å½•â€ï¼Œä¸»é¢˜æ˜¯ï¼šæˆ‘ä»¬æ˜¯å¦è¦æŠŠç°æœ‰å•ä½“æœåŠ¡æ‹†åˆ†ä¸ºå¾®æœåŠ¡ã€‚

éœ€æ±‚æè¿°:
1) èƒŒæ™¯ï¼šç›®å‰å•ä½“å·²å‡ºç°éƒ¨ç½²é¢‘ç‡ä½ã€å‘å¸ƒé£é™©å¤§ã€éƒ¨åˆ†æ¨¡å—æ€§èƒ½ç“¶é¢ˆ
2) ç›®æ ‡ï¼šåœ¨ä¸ç‰ºç‰²äº¤ä»˜é€Ÿåº¦çš„å‰æä¸‹ï¼Œæé«˜å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’Œæ•…éšœéš”ç¦»èƒ½åŠ›
3) çº¦æŸï¼šå›¢é˜Ÿäººæ•° 6 äººï¼›æœªæ¥ 3 ä¸ªæœˆä¸»è¦ç›®æ ‡æ˜¯ç¨³å®šäº¤ä»˜ï¼›è¿ç»´èƒ½åŠ›ä¸€èˆ¬ï¼›é¢„ç®—æœ‰é™
4) è¾“å‡ºå¿…é¡»åŒ…å«ï¼šèƒŒæ™¯ã€å†³ç­–ã€æ–¹æ¡ˆå¯¹æ¯”ã€é£é™©æ¸…å•ã€ä¸‹ä¸€æ­¥

è¯·æŒ‰â€œå…ˆç»™å‡ºåˆç¨¿ -> ä¸¥æ ¼éªŒè¯ -> åæ€ä¸è¶³ -> ä¿®è®¢ -> å†éªŒè¯â€çš„æ–¹å¼è¿­ä»£åˆ°é€šè¿‡éªŒè¯ã€‚
"""

    # å›ºå®šè®¡åˆ’ï¼šç¡®ä¿ demo ä¸€å®šä¼šèµ°åˆ° binding/llm_infer/åæ€/é‡è¯•/å¾ªç¯
    initial_plan = [
        {
            "id": "1",
            "tool": "generate_memo",
            "description": "ç”Ÿæˆåˆç‰ˆå¤‡å¿˜å½•ï¼ˆrubric å°†ç”±ç³»ç»Ÿæ¨æ–­ï¼‰",
            "is_pinned": True,
            "on_fail_strategy": "é‡è¯•",
        },
        {
            "id": "2",
            "tool": "verify_memo",
            "description": "éªŒè¯åˆç‰ˆå¤‡å¿˜å½•ï¼ˆå¤±è´¥ä¼šäº§å‡ºæŠ¥å‘Šä¾›åç»­åæ€ï¼‰",
            "is_pinned": True,
            "expectations": "passed å¿…é¡»ä¸º trueï¼Œå¦åˆ™å¿…é¡»ç»™å‡º issues",
            "on_fail_strategy": "å›é€€åˆ°æ­¥éª¤ 3",
        },
        {
            "id": "3",
            "tool": "reflect_feedback",
            "description": "åæ€éªŒè¯æŠ¥å‘Šå¹¶ç”Ÿæˆé’ˆå¯¹æ€§æ”¹è¿›æŒ‡ä»¤",
            "is_pinned": True,
            "on_fail_strategy": "é‡è¯•",
        },
        {
            "id": "4",
            "tool": "generate_memo",
            "description": "æ ¹æ®æ”¹è¿›æŒ‡ä»¤ç”Ÿæˆä¿®è®¢ç‰ˆå¤‡å¿˜å½•",
            "is_pinned": True,
        },
        {
            "id": "5",
            "tool": "verify_memo",
            "description": "éªŒè¯ä¿®è®¢ç‰ˆï¼ˆä¸é€šè¿‡åˆ™ç»§ç»­åæ€-ä¿®è®¢å¾ªç¯ï¼‰",
            "is_pinned": True,
            "expectations": "passed å¿…é¡»ä¸º trueï¼Œå¦åˆ™å¿…é¡»ç»™å‡º issues",
            "on_fail_strategy": "å›é€€åˆ°æ­¥éª¤ 3",
        },
    ]

    print("=" * 70)
    print("ğŸ§ª Adaptive Memo Demo (auto_agent)")
    print("=" * 70)

    final_answer = ""
    last_memo = ""
    last_verification: Dict[str, Any] = {}
    last_issues = []
    async for event in agent.run_stream(
        query=user_query,
        user_id="demo",
        initial_plan=initial_plan,
    ):
        et = event.get("event")
        data = event.get("data", {})

        if et == "planning":
            print(f"\nğŸ“ {data.get('message')}")

        elif et == "binding_plan":
            # ç®€è¦æ‰“å°å³å¯ï¼ˆè¯¦ç»†çœ‹ trace æŠ¥å‘Šï¼‰
            print(f"\nğŸ”— {data.get('message')}")
            print(f"   bindings_count={data.get('bindings_count')}")

        elif et == "execution_plan":
            print(f"\nğŸ“‹ å›ºå®šè®¡åˆ’å·²åŠ è½½ï¼Œsteps={len(data.get('steps', []))}")

        elif et == "stage_start":
            print(f"\nâ–¶ï¸  Step {data.get('step')}: {data.get('name')}")
            print(f"   æè¿°: {data.get('description', '')}")

        elif et == "param_build":
            # å‚æ•°æ„é€ è¯¦æƒ…
            is_loop = data.get("is_loop_execution", False)
            args_preview = data.get("args_preview", {})
            if is_loop:
                print(f"   ğŸ”„ [å¾ªç¯æ‰§è¡Œ] {args_preview.get('loop_reason', '')}")
            final_args = args_preview.get("final_args", {})
            if final_args:
                print(f"   ğŸ“¦ å‚æ•°é¢„è§ˆ:")
                for k, v in final_args.items():
                    v_str = str(v)
                    if len(v_str) > 100:
                        v_str = v_str[:100] + "..."
                    print(f"      - {k}: {v_str}")

        elif et == "stage_complete":
            status = "âœ…" if data.get("success") else "âŒ"
            print(f"   {status} {data.get('name')}")
            result = data.get("result") or {}
            if data.get("name") == "verify_memo" and isinstance(result, dict):
                passed = result.get("passed")
                issues = result.get("issues") or []
                score = result.get("score")
                last_issues = issues if isinstance(issues, list) else []
                last_verification = result.get("verification") or {}
                print(f"   éªŒè¯ç»“æœ: passed={passed}, score={score}")
                if isinstance(last_verification, dict) and last_verification:
                    rc = last_verification.get("risk_items_count")
                    rm = last_verification.get("risk_count_method")
                    if rc is not None:
                        print(f"   ğŸ“Œ é£é™©è®¡æ•°: {rc}ï¼ˆmethod={rm}ï¼‰")
                    rp = last_verification.get("risk_items_preview") or []
                    if isinstance(rp, list) and rp:
                        print("   ğŸ“Œ è¯†åˆ«åˆ°çš„é£é™©æ¡ç›®(å‰5):")
                        for x in rp[:5]:
                            print(f"      - {x}")
                if issues:
                    print(f"   â— é—®é¢˜åˆ—è¡¨:")
                    for issue in issues[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé—®é¢˜
                        print(f"      - {issue}")
            elif data.get("name") == "reflect_feedback" and isinstance(result, dict):
                feedback = result.get("feedback", "")
                if feedback:
                    print(f"   ğŸ“ åé¦ˆæ‘˜è¦: {feedback[:200]}...")
            elif data.get("name") == "generate_memo" and isinstance(result, dict):
                memo = result.get("memo", "")
                if memo:
                    last_memo = memo
                    print(f"   ğŸ“„ å¤‡å¿˜å½•é•¿åº¦: {len(memo)} å­—ç¬¦")
                    # ç»Ÿè®¡é£é™©æ¡æ•°
                    risk_lines = [ln for ln in memo.splitlines() if "é£é™©" in ln]
                    print(f"   ğŸ“Š é£é™©æ¡ç›®æ•°: {len(risk_lines)}")

        elif et == "stage_retry":
            print(f"   ğŸ”„ {data.get('message')}")

        elif et == "stage_replan":
            print(f"\nâš ï¸  è§¦å‘ replan: {data.get('trigger_reason')} | {data.get('reason')}")

        elif et == "answer":
            final_answer = data.get("answer", "")

        elif et == "done":
            trace = data.get("trace") or {}
            summary = trace.get("summary") or {}
            llm_calls = (summary.get("llm_calls") or {}).get("count", 0)
            total_tokens = (summary.get("llm_calls") or {}).get("total_tokens", 0)
            print("\n" + "=" * 70)
            print("âœ… Demo å®Œæˆ")
            print(f"- iterations: {data.get('iterations')}")
            print(f"- llm_calls: {llm_calls}")
            print(f"- total_tokens: {total_tokens}")
            print("=" * 70)

    # è¾“å‡ºæœ€ç»ˆ memo
    # è¯¥ demo çš„æœ€ç»ˆäº§ç‰©ä¼šåœ¨ state['memo'] ä¸­
    print("\n" + "=" * 70)
    print("ğŸ“„ æœ€ç»ˆç»“æœï¼ˆæœ€ç»ˆ memo æ­£æ–‡ï¼‰")
    print("=" * 70)
    if isinstance(final_answer, str) and final_answer.strip() and final_answer.strip() != "ä»»åŠ¡æ‰§è¡Œå®Œæˆ":
        # ä¼˜å…ˆæ‰“å° agent èšåˆçš„ answerï¼ˆè‹¥å·²è¿”å› memoï¼‰
        print(final_answer.strip())
    elif isinstance(last_memo, str) and last_memo.strip():
        # å¦åˆ™å…œåº•æ‰“å°æœ€åä¸€æ¬¡ generate_memo çš„è¾“å‡º
        print(last_memo.strip())
    else:
        print("ï¼ˆæœªæ•è·åˆ° memo è¾“å‡ºï¼šè¯·æ£€æŸ¥ generate_memo æ˜¯å¦è¿”å›äº† result['memo']ï¼‰")

    if last_issues:
        print("\n" + "-" * 70)
        print("â— æœ€åä¸€æ¬¡ verify å¤±è´¥åŸå› ï¼ˆissuesï¼‰")
        for x in last_issues[:20]:
            print(f"- {x}")

    await llm_client.close()


if __name__ == "__main__":
    asyncio.run(main())

